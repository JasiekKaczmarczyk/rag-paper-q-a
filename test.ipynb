{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypdf\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf_to_pages(path: str):\n",
    "    with open(path, \"rb\") as pdfFileObj:\n",
    "        pdfReader = pypdf.PdfReader(pdfFileObj)\n",
    "        # pages: dict[str, str] = {}\n",
    "        # total_length = 0\n",
    "        pages = []\n",
    "\n",
    "        for i, page in enumerate(pdfReader.pages):\n",
    "            # pages[str(i + 1)] = page.extract_text()\n",
    "            # total_length += len(pages[str(i + 1)])\n",
    "            pages.append(Document(page_content=page.extract_text()))\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = parse_pdf_to_pages(\"magvit2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Work in progress\\nLANGUAGE MODEL BEATS DIFFUSION\\n— T OKENIZER IS KEY TO VISUAL GENERATION\\nLijun Yu;:˚Jos´e Lezama:Nitesh B. Gundavarapu:Luca Versari:Kihyuk Sohn:\\nDavid Minnen:Yong Cheng:Agrim Gupta:Xiuye Gu:Alexander G. Hauptmann;\\nBoqing Gong:Ming-Hsuan Yang:Irfan Essa:David A. Ross:Lu Jiang:;\\n:Google,;Carnegie Mellon University\\nABSTRACT\\nWhile Large Language Models (LLMs) are the dominant models for generative\\ntasks in language, they do not perform as well as diffusion models on image and\\nvideo generation. To effectively use LLMs for visual generation, one crucial com-\\nponent is the visual tokenizer that maps pixel-space inputs to discrete tokens ap-\\npropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video\\ntokenizer designed to generate concise and expressive tokens for both videos and\\nimages using a common token vocabulary. Equipped with this new tokenizer, we\\nshow that LLMs outperform diffusion models on standard image and video gener-\\nation benchmarks including ImageNet and Kinetics. In addition, we demonstrate\\nthat our tokenizer surpasses the previously top-performing video tokenizer on two\\nmore tasks: (1) video compression comparable to the next-generation video codec\\n(VCC) according to human evaluations, and (2) learning effective representations\\nfor action recognition tasks.\\n1 I NTRODUCTION\\nLarge transformer-based language models, commonly referred to as LMs or LLMs, are the de facto\\nmodels for natural language generation (OpenAI, 2023; Google, 2023). Over time, LMs have ex-\\npanded their capabilities to generate content in various modalities, asserting their dominance in other\\ndomains like audio (Agostinelli et al., 2023), speech (Rubenstein et al., 2023), code generation (Li\\net al., 2023), medical applications (Singhal et al., 2023) and robotics (Zitkovich et al., 2023).\\nLMs are capable of generating images and videos. To do so, the image pixels are mapped into a\\nsequence of discrete tokens by a visual tokenizer ( c.f. Section 2). These tokens are then fed into\\nthe LM transformer, as if they were lexical words, for generative modeling. Despite notable ad-\\nvancements in employing LMs for visual generation (Esser et al., 2021; Chang et al., 2022), LMs\\nstill do not perform as well as diffusion models (Rombach et al., 2022). For instance, when evalu-\\nating on the ImageNet dataset, a gold standard benchmark for image generation, the best language\\nmodel (Lee et al., 2022) underperforms the diffusion model (Gao et al., 2023) by a substantial 48%\\nmargin (FID 3.41 vs. 1.79 when generating images at the 256 ˆ256 resolution).\\nWhy do language models lag behind diffusion models in visual generation? This paper suggests that\\na primary reason is the lack of a good visual representation, resembling our natural language system,\\nfor effectively modeling the visual world. To substantiate this hypothesis, this paper shows that,\\nwhen utilizing a good visual tokenizer, the masked language model (Devlin et al., 2019; Chang et al.,\\n2022; Yu et al., 2023a) surpasses the state-of-the-art diffusion models in terms of both generation\\nfidelity and efficiency across image and video benchmarks, given the same training data, comparable\\nmodel size, and training budget. To the best of our knowledge, this provides the first evidence that\\nlanguage models beat diffusion models on the hallmark ImageNet benchmark.\\nIt is worth emphasizing that our intention is not to assert whether the language model is superior\\nto others, but to promote the exploration of visual tokenization methods for LLMs. A fundamental\\ndifference of LLMs from other models, such as diffusion models, is that LLMs utilize a discrete\\nlatent format: tokens obtained from a visual tokenizer. We show that the values of these discrete\\nvisual tokens should not be overlooked considering their distinct advantages as follows. (1) Com-\\npatibility with LLMs. The main advantage of a token representation is that it shares the same form\\n˚Work done during a research internship at Google Research.\\n1arXiv:2310.05737v1  [cs.CV]  9 Oct 2023'),\n",
       " Document(page_content='Work in progress\\nas language tokens, making it straightforward to leverage the optimizations our community has de-\\nveloped over many years for LLMs. This includes faster training and inference speeds (Shazeer,\\n2019; Lester et al., 2021), advancements in model infrastructure (Dao et al., 2022; Du et al., 2022),\\nlearning recipes for model scaling (Brown et al., 2020; Chowdhery et al., 2022), and GPU/TPU op-\\ntimization, among other innovations. Unifying vision and language by the same token space could\\nset the stage for a true multimodal LLM that can understand, generate, and reason within our visual\\nenvironment. (2) Compressed representation. The discrete token may offer a fresh perspective\\non video compression. The visual tokens can serve as a new video compression format to reduce\\ndisk storage and bandwidth during internet transfers. Unlike compressed RGB pixels, these tokens\\ncan be fed directly into generative models, bypassing the conventional decompression and latent\\nencoding steps. This allows for faster processing in generative video applications, especially bene-\\nficial in edge computing cases. (3) Visual understanding benefits . Prior research has shown that\\nthe discrete tokens are valuable as a pre-training target in self-supervised representation learning, as\\ndiscussed in BEiT (Bao et al., 2021) and BEVT (Wang et al., 2022). Additionally, research finds\\nthat using tokens as the model inputs improves the robustness and generalization (Mao et al., 2021).\\nIn this paper, we introduce MAGVIT-v2, a video tokenizer designed to map videos (and images) into\\ncompact discrete tokens. Our model is built on the state-of-the-art video tokenizer, MAGVIT (Yu\\net al., 2023a), within the VQ-V AE framework (Van Den Oord et al., 2017). We propose two new\\ntechniques. First, a novel lookup-free quantization method enables the learning of a large vocabulary\\nthat is able to improve generation quality of the language model. Second, through extensive empir-\\nical analyses, we have identified modifications to the tokenizer that not only enhance generation\\nquality but also enable the tokenization of both images and videos using a shared vocabulary.\\nWe empirically demonstrate that our model outperforms the previously top-performing video tok-\\nenizer, MAGVIT, in three key areas. First, our model significantly improves the generation quality\\nof MAGVIT, establishing the state of the art on the common image and video benchmarks. Second,\\nuser studies indicate that its compression quality exceeds that of MAGVIT and the current video\\ncompression standard, HEVC (Sullivan et al., 2012). Moreover, it is on par with the next-generation\\nvideo codec, VVC (Bross et al., 2021). Finally, we show that, compared to MAGVIT, our new\\ntokens are stronger for video understanding tasks across two setups and three datasets. The main\\ncontributions of this work are:\\n• A new video tokenizer that outperforms the previously best-performing video tokenizer in three\\nareas: visual generation, video compression, and action recognition.\\n• A novel lookup-free quantization approach that enables improving the visual generation quality\\nof language models by learning a large vocabulary.\\n• To the best of our knowledge, the first evidence suggesting that a language model can outperform\\ndiffusion models on ImageNet when provided with the same training data, an equivalent model\\nsize, and a similar training budget.\\n• A video compressor with better quality than HEVC and VVC, at similar bit rates, according to\\nuser studies. To our knowledge, this is the first successful attempt of a visual tokenizer designed\\nfor video generation to achieve comparable results to standard codecs.\\n2 B ACKGROUND\\nLanguage Model (LM) for visual generation. LMs have been extended to generate images and\\nvideos. A visual tokenizer fis used to first map visual inputs into a sequence of discrete tokens.\\nA video VPRTˆHˆWˆ3(or image when T“1) is tokenized into a discrete representation\\nX“fpVq P t 1,2,¨¨¨, KuT1ˆH1ˆW1, where Kis the codebook (vocabulary) size of the visual\\ntokenizer. Xis flattened into a 1D token sequence obtained using raster scan ordering and then fed\\ninto an LM transformer for generative modeling.\\nTwo types of LMs are commonly used for visual generation. The Autoregressive LM (AR-LM)\\nincludes ImageGPT (Chen et al., 2020), DALL-E (Ramesh et al., 2021), Parti (Yu et al., 2022b), etc.\\nAn AR-LM predicts the next token given the previous tokens along with additional conditioning\\ninformation cusing a categorical distribution for pθpxi|xăi;cq. During inference, AR-LMs use\\nthe standard autoregressive decoding over the tokens. Finally, the tokens are converted back to pixels\\nby a decoder associated with the visual tokenizer.\\nThe Masked LM (MLM) is another type of language model for visual generation, such as:\\nMaskGIT (Chang et al., 2022), MAGVIT (Yu et al., 2023a), Phenaki (Villegas et al., 2022), and\\nMUSE (Chang et al., 2023), among others. An MLM is trained using a masked token objective (De-\\n2'),\n",
       " Document(page_content='Work in progress\\nvlin et al., 2019), where some tokens in the sequence are randomly masked and need to be predicted\\ngiven the observed tokens. Let mPt0,1unbe a random binary sequence where mJ1Pr0, n´1s.\\nThe MLM learns pθpxi| txj:mj“1,@ju;cqfor all iwhere mi“0. To generate a video or\\nimage during inference, the MLM uses the non-autoregressive decoding algorithms for images and\\nvideos (Chang et al., 2022; Yu et al., 2023a). The decoding starts with a fully masked sequence,\\nwhich is iteratively filled by repeating two steps: (1) sample the whole sequence ˆxptqfrompθgiven\\nthe non-masked tokens from the previous step, (2) re-mask the tλptq¨nutokens in ˆxptqwith the\\nlowest probability, following a decreasing masking ratio schedule λptq, according to timestamp t.\\nDenoising Diffusion Models (DDM). DDMs (Sohl-Dickstein et al., 2015; Song & Ermon, 2019)\\nare regarded as the state-of-the-art in visual generation due to their high-quality image (Dhariwal &\\nNichol, 2021; Ho et al., 2022a) and video generation (Ho et al., 2022c). For instance, DDPM (Ho\\net al., 2020) learns a denoising process parameterized as conditional Gaussian distributions over\\nimage pixels. Recently, diffusion models and language models have displayed a significant overlap.\\nRecent DDMs diffuse over latents rather than raw pixels. These latents are obtained using models\\nsimilar to the visual tokenizer used by LMs. In fact, the very first latent in diffusion, proposed\\nby Rombach et al. (2022), is derived from a visual tokenizer. Additionally, the diffusion model’s\\narchitecture has been shifting from the U-Net to the transformer architecture (Peebles & Xie, 2022).\\nConsequently, the boundaries between diffusion and language models in visual generation have\\nbecome less distinct. Yet, a fundamental difference between DDMs and LMs lies in the latent\\nformat, i.e., continuous vs. discrete. We have discussed the benefits of having discrete tokens in\\nSection 1 and will show that the proposed tokenizer improves in these aspects.\\nVisual tokenization. Visual tokenization plays an essential role in mapping pixels into a discrete\\nrepresentation suitable for generative modeling. VQ-V AE (Van Den Oord et al., 2017) is a corner-\\nstone work in image tokenization. A VQ-V AE model consists of a convolutional neural network\\n(CNN) encoder, a vector-quantization (VQ) bottleneck, and a CNN decoder. Given a video VP\\nRTˆHˆWˆ3, the VQ-V AE’s encoder Eproduces latent embeddings Z“EpVqPRT1ˆH1ˆW1ˆd.\\nEach embedding vector zPRdinZis then passed through the vector quantizer q, which assigns it\\nto the closest entry cPRdin the learned codebook embedding CPRKˆd:\\nqpzq“ci,where i“arg min\\njPt1,2,¨¨¨,Ku}z´cj}2. (1)\\nTo get discrete tokens, we drop the embedding dimension and represent Zby its indices XP\\nt1,2,¨¨¨, KuT1ˆH1ˆW1. For decoding, embeddings of all image tokens are given as input to the\\ndecoder Dto reconstruct the input ˆV“DpZq. Following VQ-V AE, VQGAN (Esser et al., 2021)\\nintroduces an adversarial loss and feature-level perceptual losses to enhance the image quality.\\nVideo tokenization is more challenging and VQGAN has been adapted to meet this purpose (Ge\\net al., 2022; Villegas et al., 2022; Yu et al., 2023a). The state of the art in video tokenization\\nis MAGVIT (Yu et al., 2023a), which introduces a better 3D architecture, an inflation technique\\nfor initialization using image pre-training, and robust training losses. With MAGVIT, the LMs\\nachieve leading generation quality across multiple video benchmarks. However, MAGVIT struggles\\nto tokenize images and often results in noticeable flickering in longer videos.\\n3 M ETHOD\\nWe introduce a new video tokenizer designed to map the spatial-temporal dynamics from a visual\\nscene into compact discrete tokens suitable for language models. Our approach builds upon the\\nstate-of-the-art video tokenizer, MAGVIT, as detailed in Yu et al. (2023a). This section highlights\\ntwo new designs: a lookup-free quantizer and a collection of enhancements to the tokenizer model.\\n3.1 L OOKUP -FREE QUANTIZER\\nAlthough the community has made great progress in developing VQ-V AEs, the relationship be-\\ntween improvements in the reconstruction quality and subsequent generation quality is still not well\\nunderstood. A common misconception is that improving reconstruction equates to improving the\\ngeneration of the language model. For example, enlarging the vocabulary can improve reconstruc-\\ntion quality. However, such improvement only extends to generation when the vocabulary size is\\nsmall, and a very large vocabulary can actually hurt the performance of the language model.\\nAs illustrated by the dashed curves in Fig. 1, the reconstruction FID, indicated by the right y-axis\\n(where a lower value is better), improves as the vocabulary size (the x-axis) increases. The orange\\n3'),\n",
       " Document(page_content='Work in progress\\nsolid curve in Fig. 1 represents the LM’s generation quality (the left y-axis). The generation FID\\ninitially improves but deteriorates for larger vocabulary. This may shed light on why the vocabulary\\nsize of most language models for visual generation is around 1-8k (Esser et al., 2021; Villegas et al.,\\n2022), which is significantly smaller than the size of natural language vocabulary, i.e. over 200k.\\nVocabulary (2^k)Generation FID↓ \\nReconstruction FID↓ 15.015.816.617.418.219.0\\n1.41.82.22.63.03.4\\n10 12 14 16VQ Reconstruction VQ Generation\\nLFQ Reconstruction LFQ Generation\\nFigure 1: Reconstruction and generation qual-\\nity curves in FID on ImageNet when scaling the\\ntokenizer’s vocabulary size with Vector Quantiza-\\ntion (VQ) and Lookup-Free Quantization (LFQ).\\nComparison is done at 128 ˆ128 resolution using\\nan MLM with 306-372M parameters.A simple trick for training a larger codebook in-\\nvolves decreasing the code embedding dimen-\\nsion when increasing the vocabulary size (Yu\\net al., 2022a). This trick captures the intuition\\nof limiting the representational capacity of indi-\\nvidual tokens, which in turn facilitates learning\\nover the distribution of a large vocabulary.\\nLookup-Free Quantization (LFQ). Moti-\\nvated by the above observation, we reduce the\\nVQ-V AE codebook’s embedding dimension to\\nzero. Formally, the codebook CPRKˆdis re-\\nplaced with an integer set Cwhere|C| “K.\\nRecall that in VQ-V AE models, the quantizer\\nmust look up all K d-dimensional embeddings\\nin the codebook, where dis typically 256, when\\ncomputing the closest codebook entry to the en-\\ncoder output. This new design eliminates the\\nneed for such embedding lookup entirely hence\\nwe call it lookup-free quantization (LFQ) . We found that LFQ can grow the vocabulary size in a way\\nbenefiting the generation quality of language models. As shown by the blue curves in Fig. 1, both\\nreconstruction and generation consistently improves as the vocabulary size increases – a property\\nnot observed in current VQ-V AE methods.\\nWhile various LFQ methods are available, this paper discusses a straightforward variant that as-\\nsumes independent codebook dimensions and binary latents. Specifically, the latent space of LFQ is\\ndecomposed as the Cartesian product of single-dimensional variables, as C“Ślog2K\\ni“1Ci. Given a\\nfeature vector zPRlog2K, each dimension of the quantized representation qpzqis obtained from:\\nqpziq“Ci,j,where j“arg mink}zi´Ci,k}, (2)\\nwhere Ci,jis the j-th value in Ci. With Ci“t´ 1,1u, the arg min can be computed by the sign\\nfunction as\\nqpziq“signpziq“´ 1tziď0u` 1tzią0u. (3)\\nWith LFQ, the token index for qpzqis given by:\\nIndexpzq“log2Kÿ\\ni“1arg mink}zi´Ci,k}i´1ź\\nb“0|Cb|“log2Kÿ\\ni“12i´11tzią0u, (4)\\nwhere|C0|“1sets the virtual basis.\\nWe add an entropy penalty during training to encourage codebook utilization:\\nLentropy“ErHpqpzqqs´ HrEpqpzqqs. (5)\\nThis penalty is inspired by a similar loss used in image VQGAN model (Chang et al., 2022), which is\\nalso found in entropy-based clustering (Jansen et al., 2020). In LFQ, given the independence among\\ndimensions, we rewrite Hpqpzqq“řlog2K\\ni“1Hpqpziqq. The HrEpqpzqqsterm can be approximated\\nwith sub-groups of dimensions for Ką218where direct estimation is memory bound.\\nWe note that there are various other variants of LFQ, e.g., opting for the multivariant over the binary\\ncodebook Cior employing other quantization techniques such as Agustsson et al. (2019). As the first\\npaper to introduce this concept, we focus on the simplest form with independent binary dimensions,\\nwhich shows promising improvements. Other LFQ methods merit further research.\\n3.2 V ISUAL TOKENIZER MODEL IMPROVEMENT\\nJoint image-video tokenization. A desirable feature of visual tokenization is the capability to\\ntokenize images and videos using a shared codebook. However, the MAGVIT tokenizer, which\\nutilizes the 3D CNN, faces challenges in tokenizing images due to the temporal receptive field.\\n4'),\n",
       " Document(page_content=\"Work in progress\\n… … \\n… Patch \\nEmb \\nPatch \\nEmb \\nPatch \\nEmb … \\n… … Causal \\n3D CNN \\n2D \\nCNN Quantizer 3D \\nCNN \\nCausal Transformer \\nQuantizer 3D \\nCNN 3D \\nCNN v0 \\nv'\\n0 replicate \\nvN-4:N-1 v0:N-1 v1:4 \\nx0 xN-4:N-1 x1:4 \\nCausal Transformer \\nQuantizer 3D \\nCNN 3D \\nCNN vN-4:N-1 v1:4 \\nz0 zN-4:N-\\n1 z1:4 … \\n… x0 xN-4:N-1 x1:4 \\nv0 Spatial Transformer \\nQuantizer v0 vN-2:N-1 v1:2 \\nx0 xN-2:N-1 x1:2 Causal Transformer Causal conv. \\nin time \\n(a) C-ViViT (b) C-ViViT + MAGVIT (c) Causal 3D CNN \\nFigure 2: Causal tokenizer architecture comparison . The decoders, which are omitted from the\\nfigure, employ an architecture that is symmetric to the encoder.\\nTo build a joint image-video tokenizer, a new design is needed. We begin our discussion by revis-\\niting an existing method C-ViViT (Villegas et al., 2022). As depicted in Fig. 2a, C-ViViT employs\\nfull spatial transformer blocks combined with causal temporal transformer blocks. This approach\\nperforms reasonably well but has two drawbacks. First, unlike CNNs, the positional embeddings\\nmakes it difficult to tokenize spatial resolutions that were not seen during training. Second, empiri-\\ncally we found that 3D CNNs perform better than spatial transformer and produce tokens with better\\nspatial causality of the corresponding patch.\\nTo tackle these drawbacks, we explore two plausible designs. Fig. 2b combines C-ViViT and\\nMAGVIT. Assuming a temporal compression ratio of 4, a 3D CNN processes blocks of 4 frames\\nfollowed by a causal transformer. In Fig. 2c, we use the temporally causal 3D convolution to replace\\nthe regular 3D CNN. Specifically, the temporal padding scheme for a regular 3D convolution layer\\nwith kernel size pkt, kh, kwqincludes tkt´1\\n2uframes before and tkt\\n2uframes after the input frames.\\nIn contrast, a causal 3D convolution layer pads with kt´1frames before the input and nothing after,\\nso that the output for each frame only depends on the previous frames. In consequence, the first\\nframe is always independent of other frames, allowing the model to tokenize single images.\\nTemporal convolutional subsampling with stride sis sufficient for sˆdown-sampling by mapping\\n1`sˆtframes into 1`t. After a regular sˆup-sampling, we drop the first s´1resulting frames,\\nwhich maps 1`tframes into 1`sˆtand allows for the tokenization of a single image. Tab. 5a\\nempirically compares the designs in Fig. 2, and we find that the causal 3D CNN performs the best.\\nArchitecture modifications. In addition to using causal 3D CNN layers, we made several other\\narchitectural modifications to improve upon the MAGVIT model. First, we change the encoder\\ndownsamplers from average pooling into strided convolutions to leverage learned kernels, and re-\\nplace the decoder upsamplers from nearest resizing followed by convolution with a depth-to-space\\noperator. Second, we defer the temporal downsampling from the first few encoder blocks to the last\\nones. In addition, the downsampling layer in the discriminator now utilizes 3D blur pooling (Zhang,\\n2019) to encourage shift invariance. Finally, we add one adaptive group normalization layer before\\nthe residual blocks at each resolution in the decoder to pass in the quantized latents as the control\\nsignal following StyleGAN (Karras et al., 2019). Tabs. 5b and 5c empirically verify these designs.\\nToken factorization for efficient prediction. The output tokens can be fed into language models\\nto generate videos. To assist smaller transformers predicting in a large vocabulary, we can factorize\\nthe LFQ token’s latent space into equal subspaces. For instance, rather than predicting using a\\ncodebook of size 218, we can predict in two concatenated codebooks, each of size 29. We embed\\neach subspace token separately and use their embedding summation as the token embedding for\\nthe transformer input. For the output layer with weight tying (Press & Wolf, 2017), we use the\\nembedding matrix for each subspace to obtain logits with seperate prediction heads.\\n4 E XPERIMENTS\\nThis section empirically verifies the proposed tokenizer across three distinct tasks: video and\\nimage generation, video compression, and action recognition. Fig. 3 visually compares the re-\\nconstruction quality of our tokenizer with prior works. More qualitative samples are shown at\\nhttps://magvit.cs.cmu.edu/v2 .\\n5\"),\n",
       " Document(page_content='Work in progress\\n1024x1328 0.1167 0.1665 \\n512x768 \\nOurs (ImageNet) VQGAN (ImageNet) 0.1082 \\nOurs (Web images) \\nLPIPS ↓ =\\nLPIPS ↓ =\\n0.1349 0.0788 0.0726 \\nOriginal \\nFigure 3: Image reconstruction samples with different tokenizers . We compare the VQGAN\\nused in MaskGIT (Chang et al., 2022) with two of our models trained on ImageNet and web im-\\nages (Chen et al., 2022). Original images are by Eric TERRADE and Barth Bailey on Unsplash.\\n4.1 E XPERIMENTAL SETUPS\\nDatasets. We use Kinetics-600 (K600) (Carreira et al., 2018) and UCF-101 (Soomro et al., 2012)\\nfor video generation experiments, along with ImageNet (Deng et al., 2009) for image generaton. In\\naddition, MCL-JCV (Wang et al., 2016) is used as the testbed for video compression, with Kinetics-\\n400 (K400) (Kay et al., 2017) and SSv2 (Goyal et al., 2017) for video understanding.\\nImplementation details We follow the tokenizer training setting and hyperparameters in (Yu\\net al., 2023a), unless stated otherwise. LFQ is used, which eliminates the codebook embedding,\\nto increase the default codebook size to K“218. The weight of Lentropy follows an annealing\\nschedule with a 3ˆhigher starting point and linearly decays to a fixed value of 0.1within 2k steps.\\nWe defer details regarding the evaluation setup of each subsection to the Appendix.\\n4.2 V ISUAL GENERATION\\nThe masked language model (MLM) (Devlin et al., 2019) is used in image and video generation.\\nTo verify the tokenizer, we employ the same MLM transformers in MAGVIT (Yu et al., 2023a).\\nAs we use a smaller MLM ( „300M parameters) with a large codebook ( 218«262K), the token\\nfactorization as discussed in Section 3.2 is applied using two heads with each predicting from a\\ncodebook of size 29.\\nVideo generation. We consider two standard video benchmarks, UCF-101 for class-conditional\\ngeneration and K600 for frame prediction with 5-frame condition. FVD (Unterthiner et al., 2018) is\\nused as our primary evaluation metric. Tab. 1 shows that our model surpasses all prior arts in both\\nbenchmarks. Specifically, it outperforms the previous best model MAGVIT by a large margin, while\\nusing the same MLM transformer backbone. These results demonstrate the essential role of a good\\nvisual tokenizer in enabling LMs to generate high-quality videos. Fig. 4 shows qualitative samples\\nfrom the model.\\nImage generation on ImageNet. We evaluate MAGVIT-v2 on image generation under the stan-\\ndard ImageNet class-conditional setting. We present results for resolution 512 ˆ512 in Tab. 2 and\\n6'),\n",
       " Document(page_content='Work in progress\\nTable 1: Video generation results : frame prediction on Kinetics-600 and class-conditional genera-\\ntion on UCF-101. We adopt the evaluation protocol of MAGVIT.\\nType Method K600 FVD ÓUCF FVDÓ#Params #Steps\\nGAN TrIVD-GAN-FP (Luc et al., 2020) 25.7 ˘0.7 1\\nDiffusion Video Diffusion (Ho et al., 2022c) 16.2 ˘0.3 1.1B 256\\nDiffusion RIN (Jabri et al., 2023) 10.8 411M 1000\\nAR-LM + VQ TATS (Ge et al., 2022) 332 ˘18 321M 1024\\nMLM + VQ Phenaki (Villegas et al., 2022) 36.4 ˘0.2 227M 48\\nMLM + VQ MAGVIT (Yu et al., 2023a) 9.9 ˘0.3 76˘2 306M 12\\nMLM + LFQ MAGVIT-v2 (this paper)5.2˘0.2307M12\\n4.3˘0.1 58˘3 24\\nTable 2: Image generation results : class-conditional generation on ImageNet 512 ˆ512. Guidance\\nindicates the classifier-free diffusion guidance (Ho & Salimans, 2021).˚indicates usage of extra\\ntraining data. We adopt the evaluation protocol and implementation of ADM.\\nType Methodw/o guidance w/ guidance#Params #StepsFIDÓ ISÒ FIDÓISÒ\\nGAN StyleGAN-XL (Sauer et al., 2022) 2.41 267.8 168M 1\\nDiff. + V AE˚DiT-XL/2 (Peebles & Xie, 2022) 12.03 105.3 3.04 240.8 675M 250\\nDiffusion ADM+Upsample (Dhariwal & Nichol, 2021) 9.96 121.8 3.85 221.7 731M 2000\\nDiffusion RIN (Jabri et al., 2023) 3.95 216.0 320M 1000\\nDiffusion simple diffusion (Hoogeboom et al., 2023) 3.54 205.3 3.02 248.7 2B 512\\nDiffusion VDM++ (Kingma & Gao, 2023) 2.99 232.2 2.65 278.1 2B 512\\nMLM + VQ MaskGIT (Chang et al., 2022) 7.32 156.0 227M 12\\nMLM + VQ DPC+Upsample (Lezama et al., 2023) 3.62 249.4 619M 72\\nMLM + LFQ MAGVIT-v2 (this paper)4.61 192.4307M12\\n3.07 213.1 1.91 324.3 64\\nrefer to the Appendix for 256 ˆ256 results. FID (Heusel et al., 2017) and Inception Score (IS) (Sali-\\nmans et al., 2016) are used as evaluation metrics. Our model surpasses the best performing diffusion\\nmodels both in sampling quality (w.r.t. FID and IS), and inference-time efficiency (w.r.t. sampling\\nsteps).\\nIt is worth noting that all the models compared are trained using the same ImageNet training data,\\nwith a comparable model size and training budget. Therefore, the performance primarily evaluates\\nthe model’s capabilities. The masked language model, equipped with our tokenizer, exhibits a no-\\ntable improvement in FID over the best diffusion model baseline at 512 ˆ512 (FID=1.91 vs. 2.65,\\n28%Ó). While this margin narrows at 256 ˆ256 resolution, the MLM uses a 50% reduced model\\nsize and needs much fewer decoding steps ( e.g., 64 vs. 250) to get the image generation quality.\\nQualitative samples in comparison with other models are shown in Fig. 5.\\n4.3 V IDEO COMPRESSION\\nbits per pixel (bpp)Elo score ↑\\n14001600180020002200\\n0.02 0.04 0.06 0.08 0.10 0.12Ours MAGVIT VVC HEVC\\nFigure 6: Video compression rater study .We conduct a subjective rater study to assess\\nthe compression quality of MAGVIT-v2. The\\nstudy is conducted on the 30 videos of the\\nMCL-JCV dataset, resized to a resolution of\\n640ˆ360. Sixteen raters are engaged, each pro-\\nviding responses to an average of roughly 800\\npairwise-preference questions.\\nWe calculate Elo scores (Elo & Sloan, 2008)\\nbased on pairwise preferences to quantify the\\nrelative visual quality between the models. The\\nstudy compares our model with MAGVIT as\\nwell as the current video compression standard\\nHEVC (H.265) video codec (Sullivan et al.,\\n7'),\n",
       " Document(page_content='Work in progress\\nCondition → Generation \\nFigure 4: Frame prediction samples on Kinetics-600 .\\nsimple diffusion \\nOurs \\nMaskGIT \\nStyleGAN-XL \\n VDM++ \\n DPC \\n ADM+Upsample \\nFigure 5: Class-conditional generation samples on ImageNet 512 ˆ512. We compare with each\\nof the previous works with a random sample from the same image class.\\n2012) and the next-generation codec VVC (H.266) (Bross et al., 2021). As shown in Fig. 6, raters\\nprefer our model to the compared methods at multiple bit rates.\\nTable 3: Video compression metrics .\\nMethod LPIPS ÓPSNRÒMS-SSIMÒ\\nHEVC (Sullivan et al., 2012) 0.199 30.10 0.943\\nVVC (Bross et al., 2021) 0.153 32.65 0.966\\nMAGVIT (Yu et al., 2023a) 0.144 23.70 0.846\\nMAGVIT-v2 (this paper) 0.104 26.18 0.894We also compare the compression quality us-\\ning common distortion metrics (LPIPS, PSNR,\\nand MS-SSIM) at 0.0384 bpp, the bit rate of\\nMAGVIT. The results in Tab. 3 show that our\\nmodel outperforms MAGVIT on all metrics,\\nand it outperforms all methods on LPIPS, a\\nmetric which correlates more closely with sub-\\njective quality assessments than PSNR or MS-\\nSSIM.\\n4.4 V IDEO UNDERSTANDING\\nTable 4: Video action recognition performance\\n(classification accuracy Òˆ100).\\nToken as transformer’s: Output Input\\nTokenizer SSv2 SSv2 K400 K600\\n3D VQ-V AE 64.13 41.27 44.44 45.67\\nMAGVIT (Yu et al., 2023a) 67.22 57.34 72.29 74.65\\nMAGVIT-v2 (this paper) 67.38 62.40 75.34 77.93\\nRaw pixel n/a 63.08 76.13 78.92In this subsection, we assess the tokenizer’s ca-\\npability to learn a video understanding model\\nfor action recognition. Two setups are exam-\\nined: (1) using tokens as prediction targets for\\nthe transformer’s output, and (2) using tokens\\nas the input to the transformer. For the former\\nsetup, we use a similar architecture following\\nthe BEVT (Wang et al., 2022) pre-training. For\\nthe tokens as inputs, to work with the ViViT\\nbackbone (Arnab et al., 2021), we detokenize the tokens to pixels before feeding them to the ViViT\\ntransformers.\\nTab. 4 shows that MAGVIT-v2 outperforms the previous best MAGVIT in these evaluations. Specif-\\nically, when using the decoded tokens as input, the performance approaches that of the model trained\\nwith ground-truth pixels using the same ViViT backbone. While these numbers are still worse than\\nthe state-of-the-art in action recognition, they represent solid improvements credited to the new\\ntokenizer.\\n8'),\n",
       " Document(page_content='Work in progress\\nTable 5: Ablation study verifying key design choices .\\n(a) Causal architectures on UCF-101.\\nFID is calculated on the first frame.\\n#Params FIDÓFVDÓ\\nMAGVIT 39M n/a 107.15\\nC-ViViT 90M 28.02 437.54\\nC-ViViT + MAGVIT 67M 13.52 316.70\\nMAGVIT-v2 :\\nCausal 3D CNN58M 7.06 96.33(b) Image tokenization on\\nImageNet 128 ˆ128.\\nFIDÓLPIPSÓ\\nMAGVIT 2.65 0.1292\\n+ LFQ 2.48 0.1182\\n+ large vocabulary 1.34 0.0821\\n+ up/downsampler 1.21 0.0790\\n+ deeper model 1.20 0.0686\\n+ adaptive normalization 1.15 0.0685(c) Video tokenization on UCF-101.\\nFVDÓLPIPSÓ\\nMAGVIT 24.55 0.0988\\n+ LFQ & large vocabulary 16.12 0.0694\\n+ up/downsampler 15.37 0.0678\\n+ late temporal downsample 11.11 0.0653\\n+ deeper model 8.90 0.0542\\n+ 3D blur pooling 8.62 0.0537\\n4.5 A BLATION STUDY\\nIn Fig. 1, we have ablated LFQ vs. VQ and the vocabulary size. In Tab. 5, we validate the key designs\\nproposed in Section 3.2. Specifically, Tab. 5a compares the architecture illustrated in Fig. 2; Tab. 5b\\nand Tab. 5c verify the LFQ and other improvements on ImageNet and UCF-101, respectively.\\n5 R ELATED WORK\\nVisual tokenization. Beyond the VQ-V AE models discussed in Section 2, additional models have\\nbeen proposed. ViT-VQGAN (Yu et al., 2022a) introduces transformer blocks as a substitute for\\nCNNs for image tokenization. C-ViViT (Villegas et al., 2022) further extends this idea for video to-\\nkenization. Early studies on video tokenization treat frames as independent images with no temporal\\ncompression (Wu et al., 2022; Gupta et al., 2022). Later research (Yan et al., 2021; Ge et al., 2022;\\nYu et al., 2023a) integrates 3D CNNs to tokenize spatial-temporal volumes. Despite these advances\\nin vector quantization (VQ), the codebook learned by previous VQ models is relatively small ( e.g.,\\n8k) due to the difficulty in improving the generation quality with larger vocabularies. In contrast,\\nour tokenizer can induce a large vocabulary ( e.g.,262k) that can be effectively modeled by an LM,\\nleading to enhanced image and video generation quality.\\nText-to- {image, video }.Text-to-image and text-to-video generation has garnered significant\\nrapid advancements using both language models (Yu et al., 2023b; Chang et al., 2023) and dif-\\nfusion models (Ho et al., 2022a; Blattmann et al., 2023; Singer et al., 2022; Ge et al., 2023; Ramesh\\net al., 2022). Although diffusion models, such as Midjourney, are considered the top performers in\\nthese tasks, it is unclear whether their advantage stems from the model, data, or some other uniden-\\ntified factors. Indeed, it is challenging to scientifically compare these text-to-image models as they\\nare trained on varied datasets, with some even being proprietary data, under inconsistent training\\nconditions. To facilitate a fairer comparison, this paper prioritizes using the ImageNet and Kinetics\\nbenchmarks.\\nDiffusion models. Exhibiting high quality sampling, pixel-space diffusion models (Sohl-\\nDickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020) raised to the top of the generative\\nmodeling space for both image (Ho et al., 2020; Dhariwal & Nichol, 2021; Saharia et al., 2022) and\\nvideo (Ho et al., 2022c;a; Singer et al., 2022) synthesis. The pixel-space denoising diffusion models\\n(DDMs) are later refined by the latent-space DDM (Rombach et al., 2022), which conducts diffusion\\nover the continuous latent embeddings derived from a pre-trained variational autoencoder (V AE).\\nBinary latents for image modeling were used in Wang et al. (2023), where the diffusion process is\\nparameterized with Bernoulli distributions. Recent studies have identified advantages in substituting\\nthe U-Net (Ronneberger et al., 2015) denoising backbone with a Transformer (Peebles & Xie, 2022;\\nJabri et al., 2023) or a hybrid of both (Hoogeboom et al., 2023), making the distinctions between\\ndiffusion and language models in visual generation more blurred, with a key distinction being their\\nlatent format — continuous for diffusion and discrete for language models.\\n6 C ONCLUSION AND FUTURE WORK\\nWe introduce MAGVIT-v2, a novel video tokenizer that exploits lookup-free quantization along\\nwith architectural advancements to tokenize images and video with a shared vocabulary. The ex-\\nperiments show that our tokenizer outperforms the previously leading video tokenizer across three\\nareas: visual generation, video compression, and action recognition in videos. Our results suggest\\nthat a good visual tokenizer is key for enabling language models to excel in image and video gener-\\nation. These results demonstrate the great capabilities of LMs in visual generation, and advocate for\\nfurther exploration of advanced visual tokenization methods designed for LLMs.\\n9'),\n",
       " Document(page_content='Work in progress\\nREFERENCES\\nAndrea Agostinelli, Timo I Denk, Zal ´an Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,\\nQingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. MusicLM: Generating\\nmusic from text. arXiv:2301.11325 , 2023. 1\\nEirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, and Luc Van Gool. Gener-\\native adversarial networks for extreme learned image compression. In ICCV , 2019. 4\\nAnurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu ˇci´c, and Cordelia Schmid.\\nViViT: A video vision transformer. In ICCV , 2021. 8, 16\\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT pre-training of image transform-\\ners. In ICLR , 2021. 2, 16\\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler,\\nand Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion\\nmodels. In CVPR , 2023. 9\\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity\\nnatural image synthesis. In ICLR , 2018. 17\\nBenjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary J. Sullivan, and Jens-Rainer\\nOhm. Overview of the versatile video coding (VVC) standard and its applications. IEEE Trans-\\nactions on Circuits and Systems for Video Technology , 31(10):3736–3764, 2021. 2, 8\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. In NeurIPS , 2020. 2\\nJoao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short\\nnote about Kinetics-600. arXiv:1808.01340 , 2018. 6\\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. MaskGIT: Masked genera-\\ntive image transformer. In CVPR , 2022. 1, 2, 3, 4, 6, 7, 17\\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jos ´e Lezama, Lu Jiang, Ming-Hsuan\\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image gen-\\neration via masked generative transformers. In ICML , 2023. 2, 9\\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\\nGenerative pretraining from pixels. In ICML , 2020. 2\\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. PaLI: A jointly-scaled multilingual\\nlanguage-image model. In ICLR , 2022. 6\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\\nScaling language modeling with pathways. arXiv:2204.02311 , 2022. 2\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. Flashattention: Fast and memory-\\nefficient exact attention with io-awareness. In NeurIPS , 2022. 2\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale\\nhierarchical image database. In CVPR , 2009. 6\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In NAACL , 2019. 1, 2, 6\\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In\\nNeurIPS , 2021. 3, 7, 9, 17\\n10'),\n",
       " Document(page_content='Work in progress\\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaMs: Efficient scaling of language\\nmodels with mixture-of-experts. In ICML , 2022. 2\\nArpad E. Elo and Sam Sloan. The rating of chessplayers : past and present . Ishi Press International,\\n2008. 7, 15\\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\\nsynthesis. In CVPR , 2021. 1, 3, 4, 17\\nGustav Theodor Fechner. Elemente der psychophysik , volume 2. Breitkopf u. H ¨artel, 1860. 15\\nShanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is\\na strong image synthesizer. arXiv:2303.14389 , 2023. 1, 15, 17\\nSongwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and\\nDevi Parikh. Long video generation with time-agnostic VQGAN and time-sensitive transformer.\\nInECCV , 2022. 3, 7, 9\\nSongwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs,\\nJia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior\\nfor video diffusion models. arXiv preprint arXiv:2305.10474 , 2023. 9\\nGoogle. PaLM 2 technical report. arXiv:2305.10403 , 2023. 1\\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne West-\\nphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al.\\nThe “something something” video database for learning and evaluating visual common sense. In\\nICCV , 2017. 6\\nAgrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Mart ´ın-Mart ´ın, and Li Fei-Fei.\\nMaskViT: Masked visual pre-training for video prediction. In ICLR , 2022. 9\\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\\nGANs trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS ,\\n2017. 7\\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshops , 2021. 7,\\n15, 17\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS ,\\n2020. 3, 9\\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition\\nvideo generation with diffusion models. arXiv:2210.02303 , 2022a. 3, 9\\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Sali-\\nmans. Cascaded diffusion models for high fidelity image generation. JMLR , 23(1):2249–2281,\\n2022b. 17\\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J\\nFleet. Video diffusion models. In ICLR Workshops , 2022c. 3, 7, 9\\nEmiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for\\nhigh resolution images. In ICML , 2023. 7, 9, 17\\nAllan Jabri, David J Fleet, and Ting Chen. Scalable adaptive computation for iterative generation.\\nInICML , 2023. 7, 9, 17\\nAren Jansen, Daniel PW Ellis, Shawn Hershey, R Channing Moore, Manoj Plakal, Ashok C Popat,\\nand Rif A Saurous. Coincidence, categorization, and consolidation: Learning to recognize sounds\\nwith minimal supervision. In ICASSP , 2020. 4\\n11'),\n",
       " Document(page_content='Work in progress\\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\\nadversarial networks. In CVPR , 2019. 5\\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-\\nnarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action\\nvideo dataset. arXiv:1705.06950 , 2017. 6\\nDiederik P Kingma and Ruiqi Gao. Understanding the diffusion objective as a weighted integral of\\nelbos. arXiv:2303.00848 , 2023. 7, 17\\nDoyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and WOOK SHIN HAN. Draft-and-revise:\\nEffective image generation with contextual rq-transformer. In NeurIPS , 2022. 1, 17\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\\ntuning. In EMNLP , 2021. 2\\nJos´e Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Improved masked image generation with\\nToken-Critic. In ECCV , 2022. 17\\nJos´e Lezama, Tim Salimans, Lu Jiang, Huiwen Chang, Jonathan Ho, and Irfan Essa. Discrete\\npredictor-corrector diffusion models for image synthesis. In ICLR , 2023. 7, 15, 17\\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,\\nMarc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. StarCoder: may the source be with\\nyou! arXiv:2305.06161 , 2023. 1\\nPauline Luc, Aidan Clark, Sander Dieleman, Diego de Las Casas, Yotam Doron, Albin Cassirer,\\nand Karen Simonyan. Transformation-based adversarial video prediction on large-scale data.\\narXiv:2003.04035 , 2020. 7\\nChengzhi Mao, Lu Jiang, Mostafa Dehghani, Carl V ondrick, Rahul Sukthankar, and Irfan Essa.\\nDiscrete representations strengthen vision transformer robustness. In ICLR , 2021. 2\\nOpenAI. GPT-4 technical report. arXiv:2303.08774 , 2023. 1\\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv:2212.09748 ,\\n2022. 3, 7, 9, 17\\nOfir Press and Lior Wolf. Using the output embedding to improve language models. In EACL , 2017.\\n5\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,\\nand Ilya Sutskever. Zero-shot text-to-image generation. In ICML , 2021. 2\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\\nconditional image generation with clip latents. arXiv:2204.06125 , 2022. 9\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-\\nresolution image synthesis with latent diffusion models. In CVPR , 2022. 1, 3, 9, 17\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomed-\\nical image segmentation. In MICCAI , 2015. 9\\nPaul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zal ´an Borsos,\\nF´elix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al.\\nAudioPaLM: A large language model that can speak and listen. arXiv:2306.12925 , 2023. 1\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\\ntext-to-image diffusion models with deep language understanding. In NeurIPS , 2022. 9\\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\\nImproved techniques for training GANs. In NeurIPS , 2016. 7\\n12'),\n",
       " Document(page_content='Work in progress\\nAxel Sauer, Katja Schwarz, and Andreas Geiger. StyleGAN-XL: Scaling stylegan to large diverse\\ndatasets. In SIGGRAPH , 2022. 7, 17\\nNoam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv:1911.02150 ,\\n2019. 2\\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry\\nYang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video\\ndata. arXiv:2209.14792 , 2022. 9\\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen\\nPfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering\\nwith large language models. arXiv:2305.09617 , 2023. 1\\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\\nlearning using nonequilibrium thermodynamics. In ICML , 2015. 3, 9\\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\\nInNeurIPS , 2019. 3, 9\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human\\nactions classes from videos in the wild. arXiv:1212.0402 , 2012. 6\\nGary J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high\\nefficiency video coding (HEVC) standard. IEEE Transactions on Circuits and Systems for Video\\nTechnology , 22(12):1649–1668, 2012. 2, 7, 8\\nThomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski,\\nand Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges.\\narXiv:1812.01717 , 2018. 6\\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS ,\\n2017. 2, 3\\nRuben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang,\\nMohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable\\nlength video generation from open domain textual description. arXiv:2210.02399 , 2022. 2, 3, 4,\\n5, 7, 9\\nHaiqiang Wang, Weihao Gan, Sudeng Hu, Joe Yuchieh Lin, Lina Jin, Longguang Song, Ping Wang,\\nIoannis Katsavounidis, Anne Aaron, and C-C Jay Kuo. MCL-JCV: a JND-based H. 264/A VC\\nvideo quality assessment dataset. In ICIP , 2016. 6, 15\\nRui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang\\nJiang, Luowei Zhou, and Lu Yuan. BEVT: BERT pretraining of video transformers. In CVPR ,\\n2022. 2, 8, 16\\nZe Wang, Jiang Wang, Zicheng Liu, and Qiang Qiu. Binary latent diffusion. In CVPR , 2023. 9, 17\\nChenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N ¨UWA:\\nVisual synthesis pre-training for neural visual world creation. In ECCV , 2022. 9\\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. VideoGPT: Video generation using\\nVQ-V AE and transformers. arXiv:2104.10157 , 2021. 9\\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong\\nXu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQ-\\nGAN. In ICLR , 2022a. 4, 9\\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\\nrich text-to-image generation. arXiv:2206.10789 , 2022b. 2\\n13'),\n",
       " Document(page_content='Work in progress\\nLijun Yu, Yong Cheng, Kihyuk Sohn, Jos ´e Lezama, Han Zhang, Huiwen Chang, Alexander G\\nHauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. MAGVIT: Masked generative video\\ntransformer. In CVPR , 2023a. 1, 2, 3, 6, 7, 8, 9, 15\\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun\\nBabu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models:\\nPretraining and instruction tuning. arXiv:2309.02591 , 2023b. 9\\nRichard Zhang. Making convolutional networks shift-invariant again. In ICML , 2019. 5\\nHongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models\\nwith masked transformers. arXiv:2306.09305 , 2023. 17\\nBrianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart,\\nStefan Welker, Ayzaan Wahid, et al. RT-2: Vision-language-action models transfer web knowl-\\nedge to robotic control. In CoRL , 2023. 1\\n14'),\n",
       " Document(page_content='Work in progress\\nA I MPLEMENTATION DETAILS\\nA.1 I MAGE AND VIDEO GENERATION\\nWe set up two image tokenizers to downsample by 16 ˆand 32ˆ, where they are used for generation\\nat 256ˆ256 and 512ˆ512, respectively. In both cases, an image is represented as 16 ˆ16 tokens.\\nWe train them on the ImageNet training set for 270 epochs using a batch size of 256, both with\\n256ˆ256 images.\\nWith this tokenizer we train a Masked Language Model following Yu et al. (2023a), using the token\\nfactorization described in Section 3.2. We train for 1080 epochs in accordance with the prior best\\nmodel MDT (Gao et al., 2023), with batch size 1024 for better efficiency. For preprocessing and\\ndata augmentation, we randomly crop 80-100% of an image while keeping the aspect ratio, followed\\nby random horizontal flipping. The class label is dropped for 10% of the training batches to enable\\nclassifier-free guidance (Ho & Salimans, 2021). For unguided generation, we use temperature 30\\nfor 512ˆ512 and 15 for 256 ˆ256 in the non-autoregressive decoding. For guided generation, we\\nadopt the guidance schedule from Gao et al. (2023) with temperature scaling (Lezama et al., 2023),\\nwhere we use guidance scale 25 with temperature 15.\\nWe inflate an image tokenizer trained at 128 ˆ128 for video modeling. Different from the inflation\\nin Yu et al. (2023a), we fill in the temporally last slice to correspond to the causal padding scheme.\\nIn addition, we disable the inflation for the discriminator and train it from scratch for better stability.\\nWe train the causal video tokenizer on Kinetics-600 training set for 190 epochs with batch size 256.\\nThis tokenizer is also used in subsequent evaluations of video compression and action recognition.\\nWith the causal tokenizer producing 5 ˆ16ˆ16 tokens for a 17 ˆ128ˆ128 clip, the first 2 ˆ16ˆ16\\ntokens are provided as the condition of the first 5 frames, per the standard setup of Kinetics-600\\nframe prediction benchmark. We train the MLM transformer following Yu et al. (2023a) with token\\nfactorization for 360 epochs with batch size 256. The model is sampled with a cosine schedule using\\ntemperature 32.\\nA.2 V IDEO COMPRESSION EVALUATION\\nFigure 7: Rating interface for subjective compression evaluation .\\nTo rate the quality of the different methods, we use a two-alternative forced choice rating method-\\nology (Fechner, 1860). As this methodology produces a sequence of binary decisions, we calculate\\nElo scores (Elo & Sloan, 2008) based on pairwise preferences to quantify the relative visual quality\\nbetween the models. The study was conducted on the 30 videos of the MCL-JCV dataset (Wang\\net al., 2016), scaled down to a resolution of 640 ˆ360 pixels. Sixteen raters are engaged, each pro-\\nviding responses to an average of roughly 800 pairwise-preference questions. The questions are\\npresented with an interface that parallels the one used for the Challenge on Learned Image Com-\\n15'),\n",
       " Document(page_content='Work in progress\\nTable 6: Experimental configurations with tokens as targets .\\nConfig SSv2 Pre-Training SSv2 Fine-tuning\\ninputs pixels pixels\\ninput size 16 ˆ224ˆ224ˆ3 16 ˆ224ˆ224ˆ3\\ntargets tokens classes\\nencoder ViT-B ViT-B\\ndecoder linear linear\\nmasking block-tube (Wang et al., 2022) none\\nmasking ratio 0.75 0.0\\nmask temporal length 16 0\\nbatch size 1024 512\\ntraining epochs 800 50\\nViT sequence length 8 ˆ16ˆ16 8 ˆ16ˆ16\\noptimization\\noptimizer AdamW AdamW\\noptimizer momentum 0.9 0.9\\nlayer decay 0.75 0.75\\nweight decay 0.05 0.05\\nlearning rate schedule cosine decay cosine decay\\nwarmup epochs 40 5\\ndata augmentations\\nrandom horizontal flip true false\\nlabel smoothing 0.1 0.1\\nmixup none 0.8\\ncutmix none 1.0\\ndroppath 0.0 0.1\\ndropout 0.1 0.0\\nrandom color augmentation false false\\npression ( http://compression.cc/ ), extended to comparing videos, as shown in Fig. 7. Raters\\nare instructed to compare the two videos and are not allowed to pause the videos.\\nA.3 V IDEO UNDERSTANDING EXPERIMENTS\\nTokens as prediction targets. BEiT (Bao et al., 2021) and BEVT (Wang et al., 2022) class of\\nmodels pretrain visual encoders on pixel inputs by predicting tokens as targets in a masked-modeling\\nframework, and demonstrate state-of-the-art downstream results. We use a simplified BEVT pre-\\ntraining setup to test the effectiveness of our video tokens as targets for masked modeling. The main\\ndifference is that we drop the image-stream from pre-training and only use the video stream and for\\nthis reason, we also drop the multiple decoders completely and adopt an encoder-only architecture\\nsimilar to BEiT. Detailed pre-training and fine-tuning setup is presented in Tab. 6. In Tab. 4 of the\\nmain paper, we show that our video tokens are effective targets for masked modeling based video\\nunderstanding.\\nTokens as inputs. In Tab. 4, we show that we can re-use video understanding models trained on\\npixels using our video tokens as input, with very minimal performance drop. For this experiment,\\nwe train a factorized variant of the ViViT model (Arnab et al., 2021) on pixels, and evaluate it on de-\\ntokenized pixels from our model. We use the same hyper-parameters as used in Arnab et al. (2021)\\nwith a Base sized model operating on 32 frames of inputs at 224p resolution. For the Kinetics-600\\nexperiment, we use the same hyper-parameters as the Kinetics-400 experiments.\\nB A DDITIONAL RESULTS\\nFor better visualization, the generated video samples can be viewed at https://magvit.cs.cmu.\\nedu/v2 .\\n16'),\n",
       " Document(page_content='Work in progress\\nTable 7: Class-conditional image generation on ImageNet 256 ˆ256. Guidance indicates the\\nclassifier-free diffusion guidance (Ho & Salimans, 2021).˚indicates usage of extra training data.\\nWe adopt the evaluation protocol and implementation of ADM.\\nType Methodw/o guidance w/ guidance# Params StepsFIDÓ ISÒ FIDÓISÒ\\nGAN BigGAN-deep (Brock et al., 2018) 6.95 171.4 160M 1\\nGAN StyleGAN-XL (Sauer et al., 2022) 2.30 265.1 166M 1\\nDiff. + V AE˚LDM-4 (Rombach et al., 2022) 10.56 103.5 3.60 247.7 400M 250\\nDiff. + V AE˚DiT-XL/2 (Peebles & Xie, 2022) 9.62 121.5 2.27 278.2 675M 250\\nDiff. + BAE Binary latent diffusion (Wang et al., 2023) 8.21 162.3 172M 64\\nDiffusion ADM+Upsample (Dhariwal & Nichol, 2021) 7.49 127.5 3.94 215.8 608M 2000\\nDiff. + V AE˚MDT (Gao et al., 2023) 6.23 143.0 1.79 283.0 676M 250\\nDiff. + V AE˚MaskDiT (Zheng et al., 2023) 5.69 178.0 2.28 276.6 736M 40\\nDiffusion CDM (Ho et al., 2022b) 4.88 158.7 8100\\nDiffusion RIN (Jabri et al., 2023) 3.42 182.0 410M 1000\\nDiffusion simple diffusion (Hoogeboom et al., 2023) 2.77 211.8 2.44 256.3 2B 512\\nDiffusion VDM++ (Kingma & Gao, 2023) 2.40 225.3 2.12 267.7 2B 512\\nAR-LM + VQ VQGAN (Esser et al., 2021) 15.78 78.3 1.4B 256\\nMLM + VQ MaskGIT (Chang et al., 2022) 6.18 182.1 227M 8\\nMLM + VQ Token-Critic (Lezama et al., 2022) 4.69 174.5 368M 36\\nMLM + VQ Contextual RQ-Transformer (Lee et al., 2022) 3.41 224.6 1.4B 72\\nMLM + VQ DPC (Lezama et al., 2023) 4.45 244.8 454M 180\\nMLM + LFQ MAGVIT-v2 (this paper) 3.65 200.5 1.78 319.4 307M 64\\nWhere are the text-to-image results? We want to emphasize that our goal is to develop a video\\ntokenizer, and many of the proposed techniques are designed specifically for videos. Text-to-image\\nmay be out of the scope of our paper. We are currently training text-to-video models that require\\nconsiderable computational resources. Due to time constraints, these results are not available at the\\nmoment. We intend to add the generated videos in the next revision. However, it is important to\\nnote that comparing these text-to-image or text-to-video models scientifically is challenging. These\\nmodels were trained on different datasets, and some were even based on proprietary or non-public\\ndata, all under varying training conditions.\\n17')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Work in progress\\nLANGUAGE MODEL BEATS DIFFUSION\\n— T OKENIZER IS KEY TO VISUAL GENERATION\\nLijun Yu;:˚Jos´e Lezama:Nitesh B. Gundavarapu:Luca Versari:Kihyuk Sohn:\\nDavid Minnen:Yong Cheng:Agrim Gupta:Xiuye Gu:Alexander G. Hauptmann;\\nBoqing Gong:Ming-Hsuan Yang:Irfan Essa:David A. Ross:Lu Jiang:;\\n:Google,;Carnegie Mellon University\\nABSTRACT\\nWhile Large Language Models (LLMs) are the dominant models for generative\\ntasks in language, they do not perform as well as diffusion models on image and\\nvideo generation. To effectively use LLMs for visual generation, one crucial com-\\nponent is the visual tokenizer that maps pixel-space inputs to discrete tokens ap-\\npropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video\\ntokenizer designed to generate concise and expressive tokens for both videos and\\nimages using a common token vocabulary. Equipped with this new tokenizer, we\\nshow that LLMs outperform diffusion models on standard image and video gener-\\nation benchmarks including ImageNet and Kinetics. In addition, we demonstrate\\nthat our tokenizer surpasses the previously top-performing video tokenizer on two\\nmore tasks: (1) video compression comparable to the next-generation video codec\\n(VCC) according to human evaluations, and (2) learning effective representations\\nfor action recognition tasks.\\n1 I NTRODUCTION\\nLarge transformer-based language models, commonly referred to as LMs or LLMs, are the de facto\\nmodels for natural language generation (OpenAI, 2023; Google, 2023). Over time, LMs have ex-\\npanded their capabilities to generate content in various modalities, asserting their dominance in other\\ndomains like audio (Agostinelli et al., 2023), speech (Rubenstein et al., 2023), code generation (Li\\net al., 2023), medical applications (Singhal et al., 2023) and robotics (Zitkovich et al., 2023).\\nLMs are capable of generating images and videos. To do so, the image pixels are mapped into a\\nsequence of discrete tokens by a visual tokenizer ( c.f. Section 2). These tokens are then fed into\\nthe LM transformer, as if they were lexical words, for generative modeling. Despite notable ad-\\nvancements in employing LMs for visual generation (Esser et al., 2021; Chang et al., 2022), LMs\\nstill do not perform as well as diffusion models (Rombach et al., 2022). For instance, when evalu-\\nating on the ImageNet dataset, a gold standard benchmark for image generation, the best language\\nmodel (Lee et al., 2022) underperforms the diffusion model (Gao et al., 2023) by a substantial 48%\\nmargin (FID 3.41 vs. 1.79 when generating images at the 256 ˆ256 resolution).\\nWhy do language models lag behind diffusion models in visual generation? This paper suggests that\\na primary reason is the lack of a good visual representation, resembling our natural language system,\\nfor effectively modeling the visual world. To substantiate this hypothesis, this paper shows that,\\nwhen utilizing a good visual tokenizer, the masked language model (Devlin et al., 2019; Chang et al.,\\n2022; Yu et al., 2023a) surpasses the state-of-the-art diffusion models in terms of both generation\\nfidelity and efficiency across image and video benchmarks, given the same training data, comparable\\nmodel size, and training budget. To the best of our knowledge, this provides the first evidence that\\nlanguage models beat diffusion models on the hallmark ImageNet benchmark.\\nIt is worth emphasizing that our intention is not to assert whether the language model is superior\\nto others, but to promote the exploration of visual tokenization methods for LLMs. A fundamental\\ndifference of LLMs from other models, such as diffusion models, is that LLMs utilize a discrete\\nlatent format: tokens obtained from a visual tokenizer. We show that the values of these discrete\\nvisual tokens should not be overlooked considering their distinct advantages as follows. (1) Com-\\npatibility with LLMs. The main advantage of a token representation is that it shares the same form\\n˚Work done during a research internship at Google Research.\\n1arXiv:2310.05737v1  [cs.CV]  9 Oct 2023'),\n",
       " Document(page_content='Work in progress\\nas language tokens, making it straightforward to leverage the optimizations our community has de-\\nveloped over many years for LLMs. This includes faster training and inference speeds (Shazeer,\\n2019; Lester et al., 2021), advancements in model infrastructure (Dao et al., 2022; Du et al., 2022),\\nlearning recipes for model scaling (Brown et al., 2020; Chowdhery et al., 2022), and GPU/TPU op-\\ntimization, among other innovations. Unifying vision and language by the same token space could\\nset the stage for a true multimodal LLM that can understand, generate, and reason within our visual\\nenvironment. (2) Compressed representation. The discrete token may offer a fresh perspective\\non video compression. The visual tokens can serve as a new video compression format to reduce\\ndisk storage and bandwidth during internet transfers. Unlike compressed RGB pixels, these tokens\\ncan be fed directly into generative models, bypassing the conventional decompression and latent\\nencoding steps. This allows for faster processing in generative video applications, especially bene-\\nficial in edge computing cases. (3) Visual understanding benefits . Prior research has shown that\\nthe discrete tokens are valuable as a pre-training target in self-supervised representation learning, as\\ndiscussed in BEiT (Bao et al., 2021) and BEVT (Wang et al., 2022). Additionally, research finds\\nthat using tokens as the model inputs improves the robustness and generalization (Mao et al., 2021).\\nIn this paper, we introduce MAGVIT-v2, a video tokenizer designed to map videos (and images) into\\ncompact discrete tokens. Our model is built on the state-of-the-art video tokenizer, MAGVIT (Yu\\net al., 2023a), within the VQ-V AE framework (Van Den Oord et al., 2017). We propose two new\\ntechniques. First, a novel lookup-free quantization method enables the learning of a large vocabulary\\nthat is able to improve generation quality of the language model. Second, through extensive empir-\\nical analyses, we have identified modifications to the tokenizer that not only enhance generation\\nquality but also enable the tokenization of both images and videos using a shared vocabulary.\\nWe empirically demonstrate that our model outperforms the previously top-performing video tok-\\nenizer, MAGVIT, in three key areas. First, our model significantly improves the generation quality\\nof MAGVIT, establishing the state of the art on the common image and video benchmarks. Second,\\nuser studies indicate that its compression quality exceeds that of MAGVIT and the current video\\ncompression standard, HEVC (Sullivan et al., 2012). Moreover, it is on par with the next-generation\\nvideo codec, VVC (Bross et al., 2021). Finally, we show that, compared to MAGVIT, our new\\ntokens are stronger for video understanding tasks across two setups and three datasets. The main\\ncontributions of this work are:\\n• A new video tokenizer that outperforms the previously best-performing video tokenizer in three\\nareas: visual generation, video compression, and action recognition.\\n• A novel lookup-free quantization approach that enables improving the visual generation quality\\nof language models by learning a large vocabulary.\\n• To the best of our knowledge, the first evidence suggesting that a language model can outperform\\ndiffusion models on ImageNet when provided with the same training data, an equivalent model\\nsize, and a similar training budget.\\n• A video compressor with better quality than HEVC and VVC, at similar bit rates, according to\\nuser studies. To our knowledge, this is the first successful attempt of a visual tokenizer designed\\nfor video generation to achieve comparable results to standard codecs.\\n2 B ACKGROUND\\nLanguage Model (LM) for visual generation. LMs have been extended to generate images and\\nvideos. A visual tokenizer fis used to first map visual inputs into a sequence of discrete tokens.\\nA video VPRTˆHˆWˆ3(or image when T“1) is tokenized into a discrete representation\\nX“fpVq P t 1,2,¨¨¨, KuT1ˆH1ˆW1, where Kis the codebook (vocabulary) size of the visual\\ntokenizer. Xis flattened into a 1D token sequence obtained using raster scan ordering and then fed\\ninto an LM transformer for generative modeling.\\nTwo types of LMs are commonly used for visual generation. The Autoregressive LM (AR-LM)\\nincludes ImageGPT (Chen et al., 2020), DALL-E (Ramesh et al., 2021), Parti (Yu et al., 2022b), etc.\\nAn AR-LM predicts the next token given the previous tokens along with additional conditioning\\ninformation cusing a categorical distribution for pθpxi|xăi;cq. During inference, AR-LMs use\\nthe standard autoregressive decoding over the tokens. Finally, the tokens are converted back to pixels\\nby a decoder associated with the visual tokenizer.\\nThe Masked LM (MLM) is another type of language model for visual generation, such as:\\nMaskGIT (Chang et al., 2022), MAGVIT (Yu et al., 2023a), Phenaki (Villegas et al., 2022), and\\nMUSE (Chang et al., 2023), among others. An MLM is trained using a masked token objective (De-\\n2'),\n",
       " Document(page_content='Work in progress\\nvlin et al., 2019), where some tokens in the sequence are randomly masked and need to be predicted\\ngiven the observed tokens. Let mPt0,1unbe a random binary sequence where mJ1Pr0, n´1s.\\nThe MLM learns pθpxi| txj:mj“1,@ju;cqfor all iwhere mi“0. To generate a video or\\nimage during inference, the MLM uses the non-autoregressive decoding algorithms for images and\\nvideos (Chang et al., 2022; Yu et al., 2023a). The decoding starts with a fully masked sequence,\\nwhich is iteratively filled by repeating two steps: (1) sample the whole sequence ˆxptqfrompθgiven\\nthe non-masked tokens from the previous step, (2) re-mask the tλptq¨nutokens in ˆxptqwith the\\nlowest probability, following a decreasing masking ratio schedule λptq, according to timestamp t.\\nDenoising Diffusion Models (DDM). DDMs (Sohl-Dickstein et al., 2015; Song & Ermon, 2019)\\nare regarded as the state-of-the-art in visual generation due to their high-quality image (Dhariwal &\\nNichol, 2021; Ho et al., 2022a) and video generation (Ho et al., 2022c). For instance, DDPM (Ho\\net al., 2020) learns a denoising process parameterized as conditional Gaussian distributions over\\nimage pixels. Recently, diffusion models and language models have displayed a significant overlap.\\nRecent DDMs diffuse over latents rather than raw pixels. These latents are obtained using models\\nsimilar to the visual tokenizer used by LMs. In fact, the very first latent in diffusion, proposed\\nby Rombach et al. (2022), is derived from a visual tokenizer. Additionally, the diffusion model’s\\narchitecture has been shifting from the U-Net to the transformer architecture (Peebles & Xie, 2022).\\nConsequently, the boundaries between diffusion and language models in visual generation have\\nbecome less distinct. Yet, a fundamental difference between DDMs and LMs lies in the latent\\nformat, i.e., continuous vs. discrete. We have discussed the benefits of having discrete tokens in\\nSection 1 and will show that the proposed tokenizer improves in these aspects.\\nVisual tokenization. Visual tokenization plays an essential role in mapping pixels into a discrete\\nrepresentation suitable for generative modeling. VQ-V AE (Van Den Oord et al., 2017) is a corner-\\nstone work in image tokenization. A VQ-V AE model consists of a convolutional neural network\\n(CNN) encoder, a vector-quantization (VQ) bottleneck, and a CNN decoder. Given a video VP\\nRTˆHˆWˆ3, the VQ-V AE’s encoder Eproduces latent embeddings Z“EpVqPRT1ˆH1ˆW1ˆd.\\nEach embedding vector zPRdinZis then passed through the vector quantizer q, which assigns it\\nto the closest entry cPRdin the learned codebook embedding CPRKˆd:\\nqpzq“ci,where i“arg min\\njPt1,2,¨¨¨,Ku}z´cj}2. (1)\\nTo get discrete tokens, we drop the embedding dimension and represent Zby its indices XP\\nt1,2,¨¨¨, KuT1ˆH1ˆW1. For decoding, embeddings of all image tokens are given as input to the\\ndecoder Dto reconstruct the input ˆV“DpZq. Following VQ-V AE, VQGAN (Esser et al., 2021)\\nintroduces an adversarial loss and feature-level perceptual losses to enhance the image quality.\\nVideo tokenization is more challenging and VQGAN has been adapted to meet this purpose (Ge\\net al., 2022; Villegas et al., 2022; Yu et al., 2023a). The state of the art in video tokenization\\nis MAGVIT (Yu et al., 2023a), which introduces a better 3D architecture, an inflation technique\\nfor initialization using image pre-training, and robust training losses. With MAGVIT, the LMs\\nachieve leading generation quality across multiple video benchmarks. However, MAGVIT struggles\\nto tokenize images and often results in noticeable flickering in longer videos.\\n3 M ETHOD\\nWe introduce a new video tokenizer designed to map the spatial-temporal dynamics from a visual\\nscene into compact discrete tokens suitable for language models. Our approach builds upon the\\nstate-of-the-art video tokenizer, MAGVIT, as detailed in Yu et al. (2023a). This section highlights\\ntwo new designs: a lookup-free quantizer and a collection of enhancements to the tokenizer model.\\n3.1 L OOKUP -FREE QUANTIZER\\nAlthough the community has made great progress in developing VQ-V AEs, the relationship be-\\ntween improvements in the reconstruction quality and subsequent generation quality is still not well\\nunderstood. A common misconception is that improving reconstruction equates to improving the\\ngeneration of the language model. For example, enlarging the vocabulary can improve reconstruc-\\ntion quality. However, such improvement only extends to generation when the vocabulary size is\\nsmall, and a very large vocabulary can actually hurt the performance of the language model.\\nAs illustrated by the dashed curves in Fig. 1, the reconstruction FID, indicated by the right y-axis\\n(where a lower value is better), improves as the vocabulary size (the x-axis) increases. The orange\\n3'),\n",
       " Document(page_content='Work in progress\\nsolid curve in Fig. 1 represents the LM’s generation quality (the left y-axis). The generation FID\\ninitially improves but deteriorates for larger vocabulary. This may shed light on why the vocabulary\\nsize of most language models for visual generation is around 1-8k (Esser et al., 2021; Villegas et al.,\\n2022), which is significantly smaller than the size of natural language vocabulary, i.e. over 200k.\\nVocabulary (2^k)Generation FID↓ \\nReconstruction FID↓ 15.015.816.617.418.219.0\\n1.41.82.22.63.03.4\\n10 12 14 16VQ Reconstruction VQ Generation\\nLFQ Reconstruction LFQ Generation\\nFigure 1: Reconstruction and generation qual-\\nity curves in FID on ImageNet when scaling the\\ntokenizer’s vocabulary size with Vector Quantiza-\\ntion (VQ) and Lookup-Free Quantization (LFQ).\\nComparison is done at 128 ˆ128 resolution using\\nan MLM with 306-372M parameters.A simple trick for training a larger codebook in-\\nvolves decreasing the code embedding dimen-\\nsion when increasing the vocabulary size (Yu\\net al., 2022a). This trick captures the intuition\\nof limiting the representational capacity of indi-\\nvidual tokens, which in turn facilitates learning\\nover the distribution of a large vocabulary.\\nLookup-Free Quantization (LFQ). Moti-\\nvated by the above observation, we reduce the\\nVQ-V AE codebook’s embedding dimension to\\nzero. Formally, the codebook CPRKˆdis re-\\nplaced with an integer set Cwhere|C| “K.\\nRecall that in VQ-V AE models, the quantizer\\nmust look up all K d-dimensional embeddings\\nin the codebook, where dis typically 256, when\\ncomputing the closest codebook entry to the en-\\ncoder output. This new design eliminates the\\nneed for such embedding lookup entirely hence\\nwe call it lookup-free quantization (LFQ) . We found that LFQ can grow the vocabulary size in a way\\nbenefiting the generation quality of language models. As shown by the blue curves in Fig. 1, both\\nreconstruction and generation consistently improves as the vocabulary size increases – a property\\nnot observed in current VQ-V AE methods.\\nWhile various LFQ methods are available, this paper discusses a straightforward variant that as-\\nsumes independent codebook dimensions and binary latents. Specifically, the latent space of LFQ is\\ndecomposed as the Cartesian product of single-dimensional variables, as C“Ślog2K\\ni“1Ci. Given a\\nfeature vector zPRlog2K, each dimension of the quantized representation qpzqis obtained from:\\nqpziq“Ci,j,where j“arg mink}zi´Ci,k}, (2)\\nwhere Ci,jis the j-th value in Ci. With Ci“t´ 1,1u, the arg min can be computed by the sign\\nfunction as\\nqpziq“signpziq“´ 1tziď0u` 1tzią0u. (3)\\nWith LFQ, the token index for qpzqis given by:\\nIndexpzq“log2Kÿ\\ni“1arg mink}zi´Ci,k}i´1ź\\nb“0|Cb|“log2Kÿ\\ni“12i´11tzią0u, (4)\\nwhere|C0|“1sets the virtual basis.\\nWe add an entropy penalty during training to encourage codebook utilization:\\nLentropy“ErHpqpzqqs´ HrEpqpzqqs. (5)\\nThis penalty is inspired by a similar loss used in image VQGAN model (Chang et al., 2022), which is\\nalso found in entropy-based clustering (Jansen et al., 2020). In LFQ, given the independence among\\ndimensions, we rewrite Hpqpzqq“řlog2K\\ni“1Hpqpziqq. The HrEpqpzqqsterm can be approximated\\nwith sub-groups of dimensions for Ką218where direct estimation is memory bound.\\nWe note that there are various other variants of LFQ, e.g., opting for the multivariant over the binary\\ncodebook Cior employing other quantization techniques such as Agustsson et al. (2019). As the first\\npaper to introduce this concept, we focus on the simplest form with independent binary dimensions,\\nwhich shows promising improvements. Other LFQ methods merit further research.\\n3.2 V ISUAL TOKENIZER MODEL IMPROVEMENT\\nJoint image-video tokenization. A desirable feature of visual tokenization is the capability to\\ntokenize images and videos using a shared codebook. However, the MAGVIT tokenizer, which\\nutilizes the 3D CNN, faces challenges in tokenizing images due to the temporal receptive field.\\n4'),\n",
       " Document(page_content=\"Work in progress\\n… … \\n… Patch \\nEmb \\nPatch \\nEmb \\nPatch \\nEmb … \\n… … Causal \\n3D CNN \\n2D \\nCNN Quantizer 3D \\nCNN \\nCausal Transformer \\nQuantizer 3D \\nCNN 3D \\nCNN v0 \\nv'\\n0 replicate \\nvN-4:N-1 v0:N-1 v1:4 \\nx0 xN-4:N-1 x1:4 \\nCausal Transformer \\nQuantizer 3D \\nCNN 3D \\nCNN vN-4:N-1 v1:4 \\nz0 zN-4:N-\\n1 z1:4 … \\n… x0 xN-4:N-1 x1:4 \\nv0 Spatial Transformer \\nQuantizer v0 vN-2:N-1 v1:2 \\nx0 xN-2:N-1 x1:2 Causal Transformer Causal conv. \\nin time \\n(a) C-ViViT (b) C-ViViT + MAGVIT (c) Causal 3D CNN \\nFigure 2: Causal tokenizer architecture comparison . The decoders, which are omitted from the\\nfigure, employ an architecture that is symmetric to the encoder.\\nTo build a joint image-video tokenizer, a new design is needed. We begin our discussion by revis-\\niting an existing method C-ViViT (Villegas et al., 2022). As depicted in Fig. 2a, C-ViViT employs\\nfull spatial transformer blocks combined with causal temporal transformer blocks. This approach\\nperforms reasonably well but has two drawbacks. First, unlike CNNs, the positional embeddings\\nmakes it difficult to tokenize spatial resolutions that were not seen during training. Second, empiri-\\ncally we found that 3D CNNs perform better than spatial transformer and produce tokens with better\\nspatial causality of the corresponding patch.\\nTo tackle these drawbacks, we explore two plausible designs. Fig. 2b combines C-ViViT and\\nMAGVIT. Assuming a temporal compression ratio of 4, a 3D CNN processes blocks of 4 frames\\nfollowed by a causal transformer. In Fig. 2c, we use the temporally causal 3D convolution to replace\\nthe regular 3D CNN. Specifically, the temporal padding scheme for a regular 3D convolution layer\\nwith kernel size pkt, kh, kwqincludes tkt´1\\n2uframes before and tkt\\n2uframes after the input frames.\\nIn contrast, a causal 3D convolution layer pads with kt´1frames before the input and nothing after,\\nso that the output for each frame only depends on the previous frames. In consequence, the first\\nframe is always independent of other frames, allowing the model to tokenize single images.\\nTemporal convolutional subsampling with stride sis sufficient for sˆdown-sampling by mapping\\n1`sˆtframes into 1`t. After a regular sˆup-sampling, we drop the first s´1resulting frames,\\nwhich maps 1`tframes into 1`sˆtand allows for the tokenization of a single image. Tab. 5a\\nempirically compares the designs in Fig. 2, and we find that the causal 3D CNN performs the best.\\nArchitecture modifications. In addition to using causal 3D CNN layers, we made several other\\narchitectural modifications to improve upon the MAGVIT model. First, we change the encoder\\ndownsamplers from average pooling into strided convolutions to leverage learned kernels, and re-\\nplace the decoder upsamplers from nearest resizing followed by convolution with a depth-to-space\\noperator. Second, we defer the temporal downsampling from the first few encoder blocks to the last\\nones. In addition, the downsampling layer in the discriminator now utilizes 3D blur pooling (Zhang,\\n2019) to encourage shift invariance. Finally, we add one adaptive group normalization layer before\\nthe residual blocks at each resolution in the decoder to pass in the quantized latents as the control\\nsignal following StyleGAN (Karras et al., 2019). Tabs. 5b and 5c empirically verify these designs.\\nToken factorization for efficient prediction. The output tokens can be fed into language models\\nto generate videos. To assist smaller transformers predicting in a large vocabulary, we can factorize\\nthe LFQ token’s latent space into equal subspaces. For instance, rather than predicting using a\\ncodebook of size 218, we can predict in two concatenated codebooks, each of size 29. We embed\\neach subspace token separately and use their embedding summation as the token embedding for\\nthe transformer input. For the output layer with weight tying (Press & Wolf, 2017), we use the\\nembedding matrix for each subspace to obtain logits with seperate prediction heads.\\n4 E XPERIMENTS\\nThis section empirically verifies the proposed tokenizer across three distinct tasks: video and\\nimage generation, video compression, and action recognition. Fig. 3 visually compares the re-\\nconstruction quality of our tokenizer with prior works. More qualitative samples are shown at\\nhttps://magvit.cs.cmu.edu/v2 .\\n5\"),\n",
       " Document(page_content='Work in progress\\n1024x1328 0.1167 0.1665 \\n512x768 \\nOurs (ImageNet) VQGAN (ImageNet) 0.1082 \\nOurs (Web images) \\nLPIPS ↓ =\\nLPIPS ↓ =\\n0.1349 0.0788 0.0726 \\nOriginal \\nFigure 3: Image reconstruction samples with different tokenizers . We compare the VQGAN\\nused in MaskGIT (Chang et al., 2022) with two of our models trained on ImageNet and web im-\\nages (Chen et al., 2022). Original images are by Eric TERRADE and Barth Bailey on Unsplash.\\n4.1 E XPERIMENTAL SETUPS\\nDatasets. We use Kinetics-600 (K600) (Carreira et al., 2018) and UCF-101 (Soomro et al., 2012)\\nfor video generation experiments, along with ImageNet (Deng et al., 2009) for image generaton. In\\naddition, MCL-JCV (Wang et al., 2016) is used as the testbed for video compression, with Kinetics-\\n400 (K400) (Kay et al., 2017) and SSv2 (Goyal et al., 2017) for video understanding.\\nImplementation details We follow the tokenizer training setting and hyperparameters in (Yu\\net al., 2023a), unless stated otherwise. LFQ is used, which eliminates the codebook embedding,\\nto increase the default codebook size to K“218. The weight of Lentropy follows an annealing\\nschedule with a 3ˆhigher starting point and linearly decays to a fixed value of 0.1within 2k steps.\\nWe defer details regarding the evaluation setup of each subsection to the Appendix.\\n4.2 V ISUAL GENERATION\\nThe masked language model (MLM) (Devlin et al., 2019) is used in image and video generation.\\nTo verify the tokenizer, we employ the same MLM transformers in MAGVIT (Yu et al., 2023a).\\nAs we use a smaller MLM ( „300M parameters) with a large codebook ( 218«262K), the token\\nfactorization as discussed in Section 3.2 is applied using two heads with each predicting from a\\ncodebook of size 29.\\nVideo generation. We consider two standard video benchmarks, UCF-101 for class-conditional\\ngeneration and K600 for frame prediction with 5-frame condition. FVD (Unterthiner et al., 2018) is\\nused as our primary evaluation metric. Tab. 1 shows that our model surpasses all prior arts in both\\nbenchmarks. Specifically, it outperforms the previous best model MAGVIT by a large margin, while\\nusing the same MLM transformer backbone. These results demonstrate the essential role of a good\\nvisual tokenizer in enabling LMs to generate high-quality videos. Fig. 4 shows qualitative samples\\nfrom the model.\\nImage generation on ImageNet. We evaluate MAGVIT-v2 on image generation under the stan-\\ndard ImageNet class-conditional setting. We present results for resolution 512 ˆ512 in Tab. 2 and\\n6'),\n",
       " Document(page_content='Work in progress\\nTable 1: Video generation results : frame prediction on Kinetics-600 and class-conditional genera-\\ntion on UCF-101. We adopt the evaluation protocol of MAGVIT.\\nType Method K600 FVD ÓUCF FVDÓ#Params #Steps\\nGAN TrIVD-GAN-FP (Luc et al., 2020) 25.7 ˘0.7 1\\nDiffusion Video Diffusion (Ho et al., 2022c) 16.2 ˘0.3 1.1B 256\\nDiffusion RIN (Jabri et al., 2023) 10.8 411M 1000\\nAR-LM + VQ TATS (Ge et al., 2022) 332 ˘18 321M 1024\\nMLM + VQ Phenaki (Villegas et al., 2022) 36.4 ˘0.2 227M 48\\nMLM + VQ MAGVIT (Yu et al., 2023a) 9.9 ˘0.3 76˘2 306M 12\\nMLM + LFQ MAGVIT-v2 (this paper)5.2˘0.2307M12\\n4.3˘0.1 58˘3 24\\nTable 2: Image generation results : class-conditional generation on ImageNet 512 ˆ512. Guidance\\nindicates the classifier-free diffusion guidance (Ho & Salimans, 2021).˚indicates usage of extra\\ntraining data. We adopt the evaluation protocol and implementation of ADM.\\nType Methodw/o guidance w/ guidance#Params #StepsFIDÓ ISÒ FIDÓISÒ\\nGAN StyleGAN-XL (Sauer et al., 2022) 2.41 267.8 168M 1\\nDiff. + V AE˚DiT-XL/2 (Peebles & Xie, 2022) 12.03 105.3 3.04 240.8 675M 250\\nDiffusion ADM+Upsample (Dhariwal & Nichol, 2021) 9.96 121.8 3.85 221.7 731M 2000\\nDiffusion RIN (Jabri et al., 2023) 3.95 216.0 320M 1000\\nDiffusion simple diffusion (Hoogeboom et al., 2023) 3.54 205.3 3.02 248.7 2B 512\\nDiffusion VDM++ (Kingma & Gao, 2023) 2.99 232.2 2.65 278.1 2B 512\\nMLM + VQ MaskGIT (Chang et al., 2022) 7.32 156.0 227M 12\\nMLM + VQ DPC+Upsample (Lezama et al., 2023) 3.62 249.4 619M 72\\nMLM + LFQ MAGVIT-v2 (this paper)4.61 192.4307M12\\n3.07 213.1 1.91 324.3 64\\nrefer to the Appendix for 256 ˆ256 results. FID (Heusel et al., 2017) and Inception Score (IS) (Sali-\\nmans et al., 2016) are used as evaluation metrics. Our model surpasses the best performing diffusion\\nmodels both in sampling quality (w.r.t. FID and IS), and inference-time efficiency (w.r.t. sampling\\nsteps).\\nIt is worth noting that all the models compared are trained using the same ImageNet training data,\\nwith a comparable model size and training budget. Therefore, the performance primarily evaluates\\nthe model’s capabilities. The masked language model, equipped with our tokenizer, exhibits a no-\\ntable improvement in FID over the best diffusion model baseline at 512 ˆ512 (FID=1.91 vs. 2.65,\\n28%Ó). While this margin narrows at 256 ˆ256 resolution, the MLM uses a 50% reduced model\\nsize and needs much fewer decoding steps ( e.g., 64 vs. 250) to get the image generation quality.\\nQualitative samples in comparison with other models are shown in Fig. 5.\\n4.3 V IDEO COMPRESSION\\nbits per pixel (bpp)Elo score ↑\\n14001600180020002200\\n0.02 0.04 0.06 0.08 0.10 0.12Ours MAGVIT VVC HEVC\\nFigure 6: Video compression rater study .We conduct a subjective rater study to assess\\nthe compression quality of MAGVIT-v2. The\\nstudy is conducted on the 30 videos of the\\nMCL-JCV dataset, resized to a resolution of\\n640ˆ360. Sixteen raters are engaged, each pro-\\nviding responses to an average of roughly 800\\npairwise-preference questions.\\nWe calculate Elo scores (Elo & Sloan, 2008)\\nbased on pairwise preferences to quantify the\\nrelative visual quality between the models. The\\nstudy compares our model with MAGVIT as\\nwell as the current video compression standard\\nHEVC (H.265) video codec (Sullivan et al.,\\n7'),\n",
       " Document(page_content='Work in progress\\nCondition → Generation \\nFigure 4: Frame prediction samples on Kinetics-600 .\\nsimple diffusion \\nOurs \\nMaskGIT \\nStyleGAN-XL \\n VDM++ \\n DPC \\n ADM+Upsample \\nFigure 5: Class-conditional generation samples on ImageNet 512 ˆ512. We compare with each\\nof the previous works with a random sample from the same image class.\\n2012) and the next-generation codec VVC (H.266) (Bross et al., 2021). As shown in Fig. 6, raters\\nprefer our model to the compared methods at multiple bit rates.\\nTable 3: Video compression metrics .\\nMethod LPIPS ÓPSNRÒMS-SSIMÒ\\nHEVC (Sullivan et al., 2012) 0.199 30.10 0.943\\nVVC (Bross et al., 2021) 0.153 32.65 0.966\\nMAGVIT (Yu et al., 2023a) 0.144 23.70 0.846\\nMAGVIT-v2 (this paper) 0.104 26.18 0.894We also compare the compression quality us-\\ning common distortion metrics (LPIPS, PSNR,\\nand MS-SSIM) at 0.0384 bpp, the bit rate of\\nMAGVIT. The results in Tab. 3 show that our\\nmodel outperforms MAGVIT on all metrics,\\nand it outperforms all methods on LPIPS, a\\nmetric which correlates more closely with sub-\\njective quality assessments than PSNR or MS-\\nSSIM.\\n4.4 V IDEO UNDERSTANDING\\nTable 4: Video action recognition performance\\n(classification accuracy Òˆ100).\\nToken as transformer’s: Output Input\\nTokenizer SSv2 SSv2 K400 K600\\n3D VQ-V AE 64.13 41.27 44.44 45.67\\nMAGVIT (Yu et al., 2023a) 67.22 57.34 72.29 74.65\\nMAGVIT-v2 (this paper) 67.38 62.40 75.34 77.93\\nRaw pixel n/a 63.08 76.13 78.92In this subsection, we assess the tokenizer’s ca-\\npability to learn a video understanding model\\nfor action recognition. Two setups are exam-\\nined: (1) using tokens as prediction targets for\\nthe transformer’s output, and (2) using tokens\\nas the input to the transformer. For the former\\nsetup, we use a similar architecture following\\nthe BEVT (Wang et al., 2022) pre-training. For\\nthe tokens as inputs, to work with the ViViT\\nbackbone (Arnab et al., 2021), we detokenize the tokens to pixels before feeding them to the ViViT\\ntransformers.\\nTab. 4 shows that MAGVIT-v2 outperforms the previous best MAGVIT in these evaluations. Specif-\\nically, when using the decoded tokens as input, the performance approaches that of the model trained\\nwith ground-truth pixels using the same ViViT backbone. While these numbers are still worse than\\nthe state-of-the-art in action recognition, they represent solid improvements credited to the new\\ntokenizer.\\n8'),\n",
       " Document(page_content='Work in progress\\nTable 5: Ablation study verifying key design choices .\\n(a) Causal architectures on UCF-101.\\nFID is calculated on the first frame.\\n#Params FIDÓFVDÓ\\nMAGVIT 39M n/a 107.15\\nC-ViViT 90M 28.02 437.54\\nC-ViViT + MAGVIT 67M 13.52 316.70\\nMAGVIT-v2 :\\nCausal 3D CNN58M 7.06 96.33(b) Image tokenization on\\nImageNet 128 ˆ128.\\nFIDÓLPIPSÓ\\nMAGVIT 2.65 0.1292\\n+ LFQ 2.48 0.1182\\n+ large vocabulary 1.34 0.0821\\n+ up/downsampler 1.21 0.0790\\n+ deeper model 1.20 0.0686\\n+ adaptive normalization 1.15 0.0685(c) Video tokenization on UCF-101.\\nFVDÓLPIPSÓ\\nMAGVIT 24.55 0.0988\\n+ LFQ & large vocabulary 16.12 0.0694\\n+ up/downsampler 15.37 0.0678\\n+ late temporal downsample 11.11 0.0653\\n+ deeper model 8.90 0.0542\\n+ 3D blur pooling 8.62 0.0537\\n4.5 A BLATION STUDY\\nIn Fig. 1, we have ablated LFQ vs. VQ and the vocabulary size. In Tab. 5, we validate the key designs\\nproposed in Section 3.2. Specifically, Tab. 5a compares the architecture illustrated in Fig. 2; Tab. 5b\\nand Tab. 5c verify the LFQ and other improvements on ImageNet and UCF-101, respectively.\\n5 R ELATED WORK\\nVisual tokenization. Beyond the VQ-V AE models discussed in Section 2, additional models have\\nbeen proposed. ViT-VQGAN (Yu et al., 2022a) introduces transformer blocks as a substitute for\\nCNNs for image tokenization. C-ViViT (Villegas et al., 2022) further extends this idea for video to-\\nkenization. Early studies on video tokenization treat frames as independent images with no temporal\\ncompression (Wu et al., 2022; Gupta et al., 2022). Later research (Yan et al., 2021; Ge et al., 2022;\\nYu et al., 2023a) integrates 3D CNNs to tokenize spatial-temporal volumes. Despite these advances\\nin vector quantization (VQ), the codebook learned by previous VQ models is relatively small ( e.g.,\\n8k) due to the difficulty in improving the generation quality with larger vocabularies. In contrast,\\nour tokenizer can induce a large vocabulary ( e.g.,262k) that can be effectively modeled by an LM,\\nleading to enhanced image and video generation quality.\\nText-to- {image, video }.Text-to-image and text-to-video generation has garnered significant\\nrapid advancements using both language models (Yu et al., 2023b; Chang et al., 2023) and dif-\\nfusion models (Ho et al., 2022a; Blattmann et al., 2023; Singer et al., 2022; Ge et al., 2023; Ramesh\\net al., 2022). Although diffusion models, such as Midjourney, are considered the top performers in\\nthese tasks, it is unclear whether their advantage stems from the model, data, or some other uniden-\\ntified factors. Indeed, it is challenging to scientifically compare these text-to-image models as they\\nare trained on varied datasets, with some even being proprietary data, under inconsistent training\\nconditions. To facilitate a fairer comparison, this paper prioritizes using the ImageNet and Kinetics\\nbenchmarks.\\nDiffusion models. Exhibiting high quality sampling, pixel-space diffusion models (Sohl-\\nDickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020) raised to the top of the generative\\nmodeling space for both image (Ho et al., 2020; Dhariwal & Nichol, 2021; Saharia et al., 2022) and\\nvideo (Ho et al., 2022c;a; Singer et al., 2022) synthesis. The pixel-space denoising diffusion models\\n(DDMs) are later refined by the latent-space DDM (Rombach et al., 2022), which conducts diffusion\\nover the continuous latent embeddings derived from a pre-trained variational autoencoder (V AE).\\nBinary latents for image modeling were used in Wang et al. (2023), where the diffusion process is\\nparameterized with Bernoulli distributions. Recent studies have identified advantages in substituting\\nthe U-Net (Ronneberger et al., 2015) denoising backbone with a Transformer (Peebles & Xie, 2022;\\nJabri et al., 2023) or a hybrid of both (Hoogeboom et al., 2023), making the distinctions between\\ndiffusion and language models in visual generation more blurred, with a key distinction being their\\nlatent format — continuous for diffusion and discrete for language models.\\n6 C ONCLUSION AND FUTURE WORK\\nWe introduce MAGVIT-v2, a novel video tokenizer that exploits lookup-free quantization along\\nwith architectural advancements to tokenize images and video with a shared vocabulary. The ex-\\nperiments show that our tokenizer outperforms the previously leading video tokenizer across three\\nareas: visual generation, video compression, and action recognition in videos. Our results suggest\\nthat a good visual tokenizer is key for enabling language models to excel in image and video gener-\\nation. These results demonstrate the great capabilities of LMs in visual generation, and advocate for\\nfurther exploration of advanced visual tokenization methods designed for LLMs.\\n9'),\n",
       " Document(page_content='Work in progress\\nREFERENCES\\nAndrea Agostinelli, Timo I Denk, Zal ´an Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,\\nQingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. MusicLM: Generating\\nmusic from text. arXiv:2301.11325 , 2023. 1\\nEirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, and Luc Van Gool. Gener-\\native adversarial networks for extreme learned image compression. In ICCV , 2019. 4\\nAnurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu ˇci´c, and Cordelia Schmid.\\nViViT: A video vision transformer. In ICCV , 2021. 8, 16\\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT pre-training of image transform-\\ners. In ICLR , 2021. 2, 16\\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler,\\nand Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion\\nmodels. In CVPR , 2023. 9\\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity\\nnatural image synthesis. In ICLR , 2018. 17\\nBenjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary J. Sullivan, and Jens-Rainer\\nOhm. Overview of the versatile video coding (VVC) standard and its applications. IEEE Trans-\\nactions on Circuits and Systems for Video Technology , 31(10):3736–3764, 2021. 2, 8\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. In NeurIPS , 2020. 2\\nJoao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short\\nnote about Kinetics-600. arXiv:1808.01340 , 2018. 6\\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. MaskGIT: Masked genera-\\ntive image transformer. In CVPR , 2022. 1, 2, 3, 4, 6, 7, 17\\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jos ´e Lezama, Lu Jiang, Ming-Hsuan\\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image gen-\\neration via masked generative transformers. In ICML , 2023. 2, 9\\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\\nGenerative pretraining from pixels. In ICML , 2020. 2\\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. PaLI: A jointly-scaled multilingual\\nlanguage-image model. In ICLR , 2022. 6\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\\nScaling language modeling with pathways. arXiv:2204.02311 , 2022. 2\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. Flashattention: Fast and memory-\\nefficient exact attention with io-awareness. In NeurIPS , 2022. 2\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale\\nhierarchical image database. In CVPR , 2009. 6\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In NAACL , 2019. 1, 2, 6\\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In\\nNeurIPS , 2021. 3, 7, 9, 17\\n10'),\n",
       " Document(page_content='Work in progress\\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaMs: Efficient scaling of language\\nmodels with mixture-of-experts. In ICML , 2022. 2\\nArpad E. Elo and Sam Sloan. The rating of chessplayers : past and present . Ishi Press International,\\n2008. 7, 15\\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\\nsynthesis. In CVPR , 2021. 1, 3, 4, 17\\nGustav Theodor Fechner. Elemente der psychophysik , volume 2. Breitkopf u. H ¨artel, 1860. 15\\nShanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is\\na strong image synthesizer. arXiv:2303.14389 , 2023. 1, 15, 17\\nSongwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and\\nDevi Parikh. Long video generation with time-agnostic VQGAN and time-sensitive transformer.\\nInECCV , 2022. 3, 7, 9\\nSongwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs,\\nJia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior\\nfor video diffusion models. arXiv preprint arXiv:2305.10474 , 2023. 9\\nGoogle. PaLM 2 technical report. arXiv:2305.10403 , 2023. 1\\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne West-\\nphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al.\\nThe “something something” video database for learning and evaluating visual common sense. In\\nICCV , 2017. 6\\nAgrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Mart ´ın-Mart ´ın, and Li Fei-Fei.\\nMaskViT: Masked visual pre-training for video prediction. In ICLR , 2022. 9\\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\\nGANs trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS ,\\n2017. 7\\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshops , 2021. 7,\\n15, 17\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS ,\\n2020. 3, 9\\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition\\nvideo generation with diffusion models. arXiv:2210.02303 , 2022a. 3, 9\\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Sali-\\nmans. Cascaded diffusion models for high fidelity image generation. JMLR , 23(1):2249–2281,\\n2022b. 17\\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J\\nFleet. Video diffusion models. In ICLR Workshops , 2022c. 3, 7, 9\\nEmiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for\\nhigh resolution images. In ICML , 2023. 7, 9, 17\\nAllan Jabri, David J Fleet, and Ting Chen. Scalable adaptive computation for iterative generation.\\nInICML , 2023. 7, 9, 17\\nAren Jansen, Daniel PW Ellis, Shawn Hershey, R Channing Moore, Manoj Plakal, Ashok C Popat,\\nand Rif A Saurous. Coincidence, categorization, and consolidation: Learning to recognize sounds\\nwith minimal supervision. In ICASSP , 2020. 4\\n11'),\n",
       " Document(page_content='Work in progress\\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\\nadversarial networks. In CVPR , 2019. 5\\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-\\nnarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action\\nvideo dataset. arXiv:1705.06950 , 2017. 6\\nDiederik P Kingma and Ruiqi Gao. Understanding the diffusion objective as a weighted integral of\\nelbos. arXiv:2303.00848 , 2023. 7, 17\\nDoyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and WOOK SHIN HAN. Draft-and-revise:\\nEffective image generation with contextual rq-transformer. In NeurIPS , 2022. 1, 17\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\\ntuning. In EMNLP , 2021. 2\\nJos´e Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Improved masked image generation with\\nToken-Critic. In ECCV , 2022. 17\\nJos´e Lezama, Tim Salimans, Lu Jiang, Huiwen Chang, Jonathan Ho, and Irfan Essa. Discrete\\npredictor-corrector diffusion models for image synthesis. In ICLR , 2023. 7, 15, 17\\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,\\nMarc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. StarCoder: may the source be with\\nyou! arXiv:2305.06161 , 2023. 1\\nPauline Luc, Aidan Clark, Sander Dieleman, Diego de Las Casas, Yotam Doron, Albin Cassirer,\\nand Karen Simonyan. Transformation-based adversarial video prediction on large-scale data.\\narXiv:2003.04035 , 2020. 7\\nChengzhi Mao, Lu Jiang, Mostafa Dehghani, Carl V ondrick, Rahul Sukthankar, and Irfan Essa.\\nDiscrete representations strengthen vision transformer robustness. In ICLR , 2021. 2\\nOpenAI. GPT-4 technical report. arXiv:2303.08774 , 2023. 1\\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv:2212.09748 ,\\n2022. 3, 7, 9, 17\\nOfir Press and Lior Wolf. Using the output embedding to improve language models. In EACL , 2017.\\n5\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,\\nand Ilya Sutskever. Zero-shot text-to-image generation. In ICML , 2021. 2\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\\nconditional image generation with clip latents. arXiv:2204.06125 , 2022. 9\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-\\nresolution image synthesis with latent diffusion models. In CVPR , 2022. 1, 3, 9, 17\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomed-\\nical image segmentation. In MICCAI , 2015. 9\\nPaul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zal ´an Borsos,\\nF´elix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al.\\nAudioPaLM: A large language model that can speak and listen. arXiv:2306.12925 , 2023. 1\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\\ntext-to-image diffusion models with deep language understanding. In NeurIPS , 2022. 9\\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\\nImproved techniques for training GANs. In NeurIPS , 2016. 7\\n12'),\n",
       " Document(page_content='Work in progress\\nAxel Sauer, Katja Schwarz, and Andreas Geiger. StyleGAN-XL: Scaling stylegan to large diverse\\ndatasets. In SIGGRAPH , 2022. 7, 17\\nNoam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv:1911.02150 ,\\n2019. 2\\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry\\nYang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video\\ndata. arXiv:2209.14792 , 2022. 9\\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen\\nPfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering\\nwith large language models. arXiv:2305.09617 , 2023. 1\\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\\nlearning using nonequilibrium thermodynamics. In ICML , 2015. 3, 9\\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\\nInNeurIPS , 2019. 3, 9\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human\\nactions classes from videos in the wild. arXiv:1212.0402 , 2012. 6\\nGary J Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high\\nefficiency video coding (HEVC) standard. IEEE Transactions on Circuits and Systems for Video\\nTechnology , 22(12):1649–1668, 2012. 2, 7, 8\\nThomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski,\\nand Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges.\\narXiv:1812.01717 , 2018. 6\\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS ,\\n2017. 2, 3\\nRuben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang,\\nMohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable\\nlength video generation from open domain textual description. arXiv:2210.02399 , 2022. 2, 3, 4,\\n5, 7, 9\\nHaiqiang Wang, Weihao Gan, Sudeng Hu, Joe Yuchieh Lin, Lina Jin, Longguang Song, Ping Wang,\\nIoannis Katsavounidis, Anne Aaron, and C-C Jay Kuo. MCL-JCV: a JND-based H. 264/A VC\\nvideo quality assessment dataset. In ICIP , 2016. 6, 15\\nRui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang\\nJiang, Luowei Zhou, and Lu Yuan. BEVT: BERT pretraining of video transformers. In CVPR ,\\n2022. 2, 8, 16\\nZe Wang, Jiang Wang, Zicheng Liu, and Qiang Qiu. Binary latent diffusion. In CVPR , 2023. 9, 17\\nChenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N ¨UWA:\\nVisual synthesis pre-training for neural visual world creation. In ECCV , 2022. 9\\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. VideoGPT: Video generation using\\nVQ-V AE and transformers. arXiv:2104.10157 , 2021. 9\\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong\\nXu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQ-\\nGAN. In ICLR , 2022a. 4, 9\\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\\nrich text-to-image generation. arXiv:2206.10789 , 2022b. 2\\n13'),\n",
       " Document(page_content='Work in progress\\nLijun Yu, Yong Cheng, Kihyuk Sohn, Jos ´e Lezama, Han Zhang, Huiwen Chang, Alexander G\\nHauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. MAGVIT: Masked generative video\\ntransformer. In CVPR , 2023a. 1, 2, 3, 6, 7, 8, 9, 15\\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun\\nBabu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models:\\nPretraining and instruction tuning. arXiv:2309.02591 , 2023b. 9\\nRichard Zhang. Making convolutional networks shift-invariant again. In ICML , 2019. 5\\nHongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models\\nwith masked transformers. arXiv:2306.09305 , 2023. 17\\nBrianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart,\\nStefan Welker, Ayzaan Wahid, et al. RT-2: Vision-language-action models transfer web knowl-\\nedge to robotic control. In CoRL , 2023. 1\\n14'),\n",
       " Document(page_content='Work in progress\\nA I MPLEMENTATION DETAILS\\nA.1 I MAGE AND VIDEO GENERATION\\nWe set up two image tokenizers to downsample by 16 ˆand 32ˆ, where they are used for generation\\nat 256ˆ256 and 512ˆ512, respectively. In both cases, an image is represented as 16 ˆ16 tokens.\\nWe train them on the ImageNet training set for 270 epochs using a batch size of 256, both with\\n256ˆ256 images.\\nWith this tokenizer we train a Masked Language Model following Yu et al. (2023a), using the token\\nfactorization described in Section 3.2. We train for 1080 epochs in accordance with the prior best\\nmodel MDT (Gao et al., 2023), with batch size 1024 for better efficiency. For preprocessing and\\ndata augmentation, we randomly crop 80-100% of an image while keeping the aspect ratio, followed\\nby random horizontal flipping. The class label is dropped for 10% of the training batches to enable\\nclassifier-free guidance (Ho & Salimans, 2021). For unguided generation, we use temperature 30\\nfor 512ˆ512 and 15 for 256 ˆ256 in the non-autoregressive decoding. For guided generation, we\\nadopt the guidance schedule from Gao et al. (2023) with temperature scaling (Lezama et al., 2023),\\nwhere we use guidance scale 25 with temperature 15.\\nWe inflate an image tokenizer trained at 128 ˆ128 for video modeling. Different from the inflation\\nin Yu et al. (2023a), we fill in the temporally last slice to correspond to the causal padding scheme.\\nIn addition, we disable the inflation for the discriminator and train it from scratch for better stability.\\nWe train the causal video tokenizer on Kinetics-600 training set for 190 epochs with batch size 256.\\nThis tokenizer is also used in subsequent evaluations of video compression and action recognition.\\nWith the causal tokenizer producing 5 ˆ16ˆ16 tokens for a 17 ˆ128ˆ128 clip, the first 2 ˆ16ˆ16\\ntokens are provided as the condition of the first 5 frames, per the standard setup of Kinetics-600\\nframe prediction benchmark. We train the MLM transformer following Yu et al. (2023a) with token\\nfactorization for 360 epochs with batch size 256. The model is sampled with a cosine schedule using\\ntemperature 32.\\nA.2 V IDEO COMPRESSION EVALUATION\\nFigure 7: Rating interface for subjective compression evaluation .\\nTo rate the quality of the different methods, we use a two-alternative forced choice rating method-\\nology (Fechner, 1860). As this methodology produces a sequence of binary decisions, we calculate\\nElo scores (Elo & Sloan, 2008) based on pairwise preferences to quantify the relative visual quality\\nbetween the models. The study was conducted on the 30 videos of the MCL-JCV dataset (Wang\\net al., 2016), scaled down to a resolution of 640 ˆ360 pixels. Sixteen raters are engaged, each pro-\\nviding responses to an average of roughly 800 pairwise-preference questions. The questions are\\npresented with an interface that parallels the one used for the Challenge on Learned Image Com-\\n15'),\n",
       " Document(page_content='Work in progress\\nTable 6: Experimental configurations with tokens as targets .\\nConfig SSv2 Pre-Training SSv2 Fine-tuning\\ninputs pixels pixels\\ninput size 16 ˆ224ˆ224ˆ3 16 ˆ224ˆ224ˆ3\\ntargets tokens classes\\nencoder ViT-B ViT-B\\ndecoder linear linear\\nmasking block-tube (Wang et al., 2022) none\\nmasking ratio 0.75 0.0\\nmask temporal length 16 0\\nbatch size 1024 512\\ntraining epochs 800 50\\nViT sequence length 8 ˆ16ˆ16 8 ˆ16ˆ16\\noptimization\\noptimizer AdamW AdamW\\noptimizer momentum 0.9 0.9\\nlayer decay 0.75 0.75\\nweight decay 0.05 0.05\\nlearning rate schedule cosine decay cosine decay\\nwarmup epochs 40 5\\ndata augmentations\\nrandom horizontal flip true false\\nlabel smoothing 0.1 0.1\\nmixup none 0.8\\ncutmix none 1.0\\ndroppath 0.0 0.1\\ndropout 0.1 0.0\\nrandom color augmentation false false\\npression ( http://compression.cc/ ), extended to comparing videos, as shown in Fig. 7. Raters\\nare instructed to compare the two videos and are not allowed to pause the videos.\\nA.3 V IDEO UNDERSTANDING EXPERIMENTS\\nTokens as prediction targets. BEiT (Bao et al., 2021) and BEVT (Wang et al., 2022) class of\\nmodels pretrain visual encoders on pixel inputs by predicting tokens as targets in a masked-modeling\\nframework, and demonstrate state-of-the-art downstream results. We use a simplified BEVT pre-\\ntraining setup to test the effectiveness of our video tokens as targets for masked modeling. The main\\ndifference is that we drop the image-stream from pre-training and only use the video stream and for\\nthis reason, we also drop the multiple decoders completely and adopt an encoder-only architecture\\nsimilar to BEiT. Detailed pre-training and fine-tuning setup is presented in Tab. 6. In Tab. 4 of the\\nmain paper, we show that our video tokens are effective targets for masked modeling based video\\nunderstanding.\\nTokens as inputs. In Tab. 4, we show that we can re-use video understanding models trained on\\npixels using our video tokens as input, with very minimal performance drop. For this experiment,\\nwe train a factorized variant of the ViViT model (Arnab et al., 2021) on pixels, and evaluate it on de-\\ntokenized pixels from our model. We use the same hyper-parameters as used in Arnab et al. (2021)\\nwith a Base sized model operating on 32 frames of inputs at 224p resolution. For the Kinetics-600\\nexperiment, we use the same hyper-parameters as the Kinetics-400 experiments.\\nB A DDITIONAL RESULTS\\nFor better visualization, the generated video samples can be viewed at https://magvit.cs.cmu.\\nedu/v2 .\\n16'),\n",
       " Document(page_content='Work in progress\\nTable 7: Class-conditional image generation on ImageNet 256 ˆ256. Guidance indicates the\\nclassifier-free diffusion guidance (Ho & Salimans, 2021).˚indicates usage of extra training data.\\nWe adopt the evaluation protocol and implementation of ADM.\\nType Methodw/o guidance w/ guidance# Params StepsFIDÓ ISÒ FIDÓISÒ\\nGAN BigGAN-deep (Brock et al., 2018) 6.95 171.4 160M 1\\nGAN StyleGAN-XL (Sauer et al., 2022) 2.30 265.1 166M 1\\nDiff. + V AE˚LDM-4 (Rombach et al., 2022) 10.56 103.5 3.60 247.7 400M 250\\nDiff. + V AE˚DiT-XL/2 (Peebles & Xie, 2022) 9.62 121.5 2.27 278.2 675M 250\\nDiff. + BAE Binary latent diffusion (Wang et al., 2023) 8.21 162.3 172M 64\\nDiffusion ADM+Upsample (Dhariwal & Nichol, 2021) 7.49 127.5 3.94 215.8 608M 2000\\nDiff. + V AE˚MDT (Gao et al., 2023) 6.23 143.0 1.79 283.0 676M 250\\nDiff. + V AE˚MaskDiT (Zheng et al., 2023) 5.69 178.0 2.28 276.6 736M 40\\nDiffusion CDM (Ho et al., 2022b) 4.88 158.7 8100\\nDiffusion RIN (Jabri et al., 2023) 3.42 182.0 410M 1000\\nDiffusion simple diffusion (Hoogeboom et al., 2023) 2.77 211.8 2.44 256.3 2B 512\\nDiffusion VDM++ (Kingma & Gao, 2023) 2.40 225.3 2.12 267.7 2B 512\\nAR-LM + VQ VQGAN (Esser et al., 2021) 15.78 78.3 1.4B 256\\nMLM + VQ MaskGIT (Chang et al., 2022) 6.18 182.1 227M 8\\nMLM + VQ Token-Critic (Lezama et al., 2022) 4.69 174.5 368M 36\\nMLM + VQ Contextual RQ-Transformer (Lee et al., 2022) 3.41 224.6 1.4B 72\\nMLM + VQ DPC (Lezama et al., 2023) 4.45 244.8 454M 180\\nMLM + LFQ MAGVIT-v2 (this paper) 3.65 200.5 1.78 319.4 307M 64\\nWhere are the text-to-image results? We want to emphasize that our goal is to develop a video\\ntokenizer, and many of the proposed techniques are designed specifically for videos. Text-to-image\\nmay be out of the scope of our paper. We are currently training text-to-video models that require\\nconsiderable computational resources. Due to time constraints, these results are not available at the\\nmoment. We intend to add the generated videos in the next revision. However, it is important to\\nnote that comparing these text-to-image or text-to-video models scientifically is challenging. These\\nmodels were trained on different datasets, and some were even based on proprietary or non-public\\ndata, all under varying training conditions.\\n17')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 17/17 [00:02<00:00,  8.10it/s]\n"
     ]
    }
   ],
   "source": [
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True),\n",
    "    collection_name=\"local_rag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are AI language model assistant. Your task is to generate five different versions of the give user question to retrieve relevant documents from vector database. \n",
    "    By generating multiple perspectives on the user question, your goal is to help the user overome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(),\n",
    "    llm,\n",
    "    prompt=query_prompt,\n",
    ")\n",
    "\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 21.74it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 21.17it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 11.40it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 11.45it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 20.16it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  7.79it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 21.03it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 11.15it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 11.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main contribution of this paper is the introduction of Lookup-Free Quantization (LFQ), a novel method for growing the vocabulary size of language models that can improve the generation quality. Specifically, LFQ eliminates the need for embedding lookup in Vector Quantization (VQ) models and allows for more efficient learning over large vocabularies.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is the main contribution of this paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 21.29it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 21.37it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 11.37it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 11.17it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 20.65it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 20.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the provided context, a novel lookup-free quantization (LFQ) approach is mentioned as part of the visual generation component. This approach enables improving the visual generation quality of language models by learning a large vocabulary.\\n\\nHowever, the exact details of how LFQ works are not explicitly described in the provided text. It appears that LFQ is used to improve the quality of visual generation by a language model, but further information on its implementation or mechanics is not provided.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"How does LFQ work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
